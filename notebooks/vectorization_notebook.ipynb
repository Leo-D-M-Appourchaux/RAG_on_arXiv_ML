{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8851bc0",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbedcd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage stuff\n",
    "PROCESSED_FOLDER = \"processed_data\"\n",
    "EXTRACTION_FOLDER = \"extracted_data\"\n",
    "\n",
    "# Database stuff\n",
    "LOCAL_DB_PATH = \"mydatabase.db\"\n",
    "\n",
    "# Vectorization stuff\n",
    "VECT_MODEL_NAME = \"llamaindex/vdr-2b-multi-v1\"\n",
    "VECT_MODEL_LOCAL_PATH = \"weights_vect/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb3e99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8124cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /opt/homebrew/lib/python3.11/site-packages (0.12.37)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.4.8)\n",
      "Requirement already satisfied: llama-index-cli<0.5,>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.36 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.12.37)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.6.11)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.3.44)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.4.7)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.82.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (3.12.0)\n",
      "Requirement already satisfied: aiosqlite in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.1.2)\n",
      "Requirement already satisfied: dataclasses-json in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2025.3.0)\n",
      "Requirement already satisfied: httpx in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (11.2.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.11.5)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (2.32.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /opt/homebrew/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.36->llama-index) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.36->llama-index) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.22)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.5.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.23)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.36->llama-index) (1.20.0)\n",
      "Requirement already satisfied: griffe in /opt/homebrew/lib/python3.11/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (3.1.6)\n",
      "Requirement already satisfied: platformdirs in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (4.3.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/homebrew/lib/python3.11/site-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.4.26)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.36->llama-index) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.36->llama-index) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.36->llama-index) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.36->llama-index) (0.16.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.23 in /opt/homebrew/lib/python3.11/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.23)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.36->llama-index) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.36->llama-index) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/homebrew/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.36->llama-index) (3.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.36->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.36->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-cloud-services>=0.6.23->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.36->llama-index) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.17.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/homebrew/lib/python3.11/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.36->llama-index) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /opt/homebrew/lib/python3.11/site-packages (0.5.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.32.0)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (0.12.37)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (4.1.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.1.2)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.12.0)\n",
      "Requirement already satisfied: aiosqlite in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (2.1.2)\n",
      "Requirement already satisfied: dataclasses-json in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.2.0)\n",
      "Requirement already satisfied: httpx in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (11.2.1)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (2.11.5)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /opt/homebrew/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/lib/python3.11/site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.17.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.52.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.15.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.20.0)\n",
      "Requirement already satisfied: griffe in /opt/homebrew/lib/python3.11/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (3.1.6)\n",
      "Requirement already satisfied: platformdirs in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (4.3.8)\n",
      "Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/homebrew/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (3.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (3.26.1)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/homebrew/lib/python3.11/site-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-embeddings-huggingface) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.32.0)\n",
      "Requirement already satisfied: packaging in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leoappourchaux/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlite_vec in /opt/homebrew/lib/python3.11/site-packages (0.1.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pymupdf in /opt/homebrew/lib/python3.11/site-packages (1.26.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install datasets\n",
    "%pip install sqlite_vec\n",
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd09be7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from typing import List, Tuple, Dict, Any, Set, Optional, Deque\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from uuid import uuid4, UUID\n",
    "import concurrent.futures\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sqlite_vec\n",
    "import traceback\n",
    "import aiosqlite\n",
    "import argparse\n",
    "import tempfile\n",
    "import asyncio\n",
    "import sqlite3\n",
    "import struct\n",
    "import torch\n",
    "import fitz\n",
    "import time\n",
    "import json\n",
    "import tqdm\n",
    "import sys\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1383745",
   "metadata": {},
   "source": [
    "## Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6806e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec_version=v0.1.6\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(LOCAL_DB_PATH)\n",
    "conn.enable_load_extension(True)\n",
    "sqlite_vec.load(conn)\n",
    "conn.enable_load_extension(False)\n",
    "\n",
    "vec_version, = conn.execute(\"select vec_version()\").fetchone()\n",
    "print(f\"vec_version={vec_version}\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "\n",
    "# Create regular documents table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE documents (\n",
    "    document_id TEXT PRIMARY KEY,\n",
    "    title TEXT NOT NULL,\n",
    "    upload_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    total_pages INTEGER NOT NULL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Create regular page_images table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE page_images (\n",
    "    page_id TEXT PRIMARY KEY,\n",
    "    document_id TEXT NOT NULL,\n",
    "    page_number INTEGER NOT NULL,\n",
    "    page_text TEXT NOT NULL,\n",
    "    latex_code TEXT,\n",
    "    FOREIGN KEY (document_id) REFERENCES documents(document_id)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Create virtual table for page_images' vector data\n",
    "cursor.execute(\"\"\"\n",
    "CREATE VIRTUAL TABLE page_images_vectors USING vec0(\n",
    "    page_id TEXT PRIMARY KEY,\n",
    "    vector_data FLOAT[1536]\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Create trigger to delete from page_images_vectors when a page_image is deleted\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TRIGGER delete_page_images_vector\n",
    "AFTER DELETE ON page_images\n",
    "BEGIN\n",
    "    DELETE FROM page_images_vectors WHERE page_id = OLD.page_id;\n",
    "END;\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba8d3c",
   "metadata": {},
   "source": [
    "## Extract from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622572a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'staghado/ArXiv-tables'...\n",
      "Dataset loaded successfully! Total available samples: 1328\n",
      "Extracting 10 samples to extracted_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting samples: 100%|██████████| 10/10 [00:00<00:00, 72.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete! Successfully processed 10/10 samples.\n",
      "Files saved to: extracted_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def ensure_directory_exists(directory):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_sample(sample, index, output_dir):\n",
    "    \"\"\"Process a single sample from the dataset.\"\"\"\n",
    "    try:\n",
    "        # Extract image and save as JPG\n",
    "        img_bytes = sample[\"page_image\"]\n",
    "        img = Image.open(BytesIO(img_bytes))\n",
    "        img_path = os.path.join(output_dir, f\"sample_{index}.jpg\")\n",
    "        img.save(img_path, optimize=True)\n",
    "        \n",
    "        # Extract LaTeX content and save as TXT\n",
    "        latex_content = sample[\"latex_content\"]\n",
    "        txt_path = os.path.join(output_dir, f\"sample_{index}.txt\")\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(latex_content)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {index}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def extract_dataset(num_samples, output_dir, batch_size=4):\n",
    "    \"\"\"Extract images and LaTeX code from the ArXiv-tables dataset.\"\"\"\n",
    "    # Ensure output directory exists\n",
    "    ensure_directory_exists(output_dir)\n",
    "    \n",
    "    print(f\"Loading dataset 'staghado/ArXiv-tables'...\")\n",
    "    dataset = load_dataset(\"staghado/ArXiv-tables\", split=\"train\")\n",
    "    \n",
    "    # Determine actual number of samples to extract\n",
    "    available_samples = len(dataset)\n",
    "    num_samples = min(num_samples, available_samples)\n",
    "    \n",
    "    print(f\"Dataset loaded successfully! Total available samples: {available_samples}\")\n",
    "    print(f\"Extracting {num_samples} samples to {output_dir}...\")\n",
    "    \n",
    "    successful = 0\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "        # Create a dictionary of futures to their index\n",
    "        future_to_idx = {\n",
    "            executor.submit(process_sample, dataset[i], i, output_dir): i \n",
    "            for i in range(num_samples)\n",
    "        }\n",
    "        \n",
    "        # Process as completed with progress bar\n",
    "        for future in tqdm.tqdm(concurrent.futures.as_completed(future_to_idx), \n",
    "                               total=num_samples, \n",
    "                               desc=\"Extracting samples\"):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                if future.result():\n",
    "                    successful += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred while processing sample {idx}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Extraction complete! Successfully processed {successful}/{num_samples} samples.\")\n",
    "    print(f\"Files saved to: {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    extract_dataset(\n",
    "        num_samples=10,\n",
    "        output_dir=EXTRACTION_FOLDER,\n",
    "        batch_size=4\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc5cc4",
   "metadata": {},
   "source": [
    "## Download embedding model (around 40 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3371d9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at weights_vect/\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(VECT_MODEL_LOCAL_PATH):\n",
    "    print(f\"Downloading model to {VECT_MODEL_LOCAL_PATH}\")\n",
    "    snapshot_download(\n",
    "        repo_id=VECT_MODEL_NAME,\n",
    "        local_dir=VECT_MODEL_LOCAL_PATH,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model already exists at {VECT_MODEL_LOCAL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59d096",
   "metadata": {},
   "source": [
    "## Load embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e87606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available. Using Apple Silicon GPU.\n",
      "Loading model: weights_vect/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name='weights_vect/' embed_batch_size=10 callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x312138690> num_workers=None max_length=32768 normalize=True query_instruction=None text_instruction=None cache_folder=None show_progress_bar=False\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"MPS backend is available. Using Apple Silicon GPU.\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    try:\n",
    "        print(f\"Device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get CUDA device details: {e}\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"MPS and CUDA not available. Using CPU.\")\n",
    "\n",
    "print(f\"Loading model: {VECT_MODEL_LOCAL_PATH}\")\n",
    "model = HuggingFaceEmbedding(\n",
    "    model_name=VECT_MODEL_LOCAL_PATH,\n",
    "    device=DEVICE,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4f397",
   "metadata": {},
   "source": [
    "## Functions for vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ada741",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def embed_text(request: dict):\n",
    "    try:\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in request[\"texts\"]:\n",
    "            print(f\"Processing user query: {text}\\n\")\n",
    "            \n",
    "            query_embedding = model.get_query_embedding(text)\n",
    "            \n",
    "            if isinstance(query_embedding, np.ndarray):\n",
    "                query_embedding = query_embedding.tolist()\n",
    "            \n",
    "            embeddings.append(query_embedding)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error details: {traceback.format_exc()}\")\n",
    "        raise Exception(f\"Text embedding failed: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "async def embed_images(files: dict):\n",
    "    try:\n",
    "        if len(files) != 1:\n",
    "            raise ValueError(\"Expected exactly one file for embedding\")\n",
    "        file_tuple = list(files.values())[0]\n",
    "        _, image_bytes, _ = file_tuple\n",
    "        \n",
    "        # Save to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:\n",
    "            tmp_file.write(image_bytes)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        print(f\"Processing image from temporary file: {tmp_file_path}\")\n",
    "        \n",
    "        # Generate embedding using the file path\n",
    "        image_embedding = model.get_image_embedding(tmp_file_path)\n",
    "        \n",
    "        # Clean up the temporary file\n",
    "        os.unlink(tmp_file_path)\n",
    "        \n",
    "        if isinstance(image_embedding, np.ndarray):\n",
    "            image_embedding = image_embedding.tolist()\n",
    "        \n",
    "        return image_embedding\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error details: {traceback.format_exc()}\")\n",
    "        raise Exception(f\"Image embedding failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54e605",
   "metadata": {},
   "source": [
    "## Fill DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce31b25",
   "metadata": {},
   "source": [
    "### Functions for processing status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9384101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_estimated_time(embedding_queue, document_id, total_pages, processed_pages) -> str:\n",
    "    \"\"\"Calculate estimated completion time based on current processing rate\"\"\"\n",
    "    if document_id not in embedding_queue.document_start_times:\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    if processed_pages == 0:\n",
    "        return \"Calculating...\"\n",
    "    \n",
    "    start_time = embedding_queue.document_start_times[document_id]\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    \n",
    "    # Calculate pages per second\n",
    "    pages_per_second = processed_pages / elapsed_time\n",
    "    \n",
    "    if pages_per_second <= 0:\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    # Calculate remaining time\n",
    "    remaining_pages = total_pages - processed_pages\n",
    "    remaining_seconds = remaining_pages / pages_per_second\n",
    "    \n",
    "    # Format remaining time\n",
    "    if remaining_seconds < 60:\n",
    "        return f\"Less than a minute\"\n",
    "    elif remaining_seconds < 3600:\n",
    "        minutes = int(remaining_seconds / 60)\n",
    "        return f\"About {minutes} minute{'s' if minutes > 1 else ''}\"\n",
    "    else:\n",
    "        hours = int(remaining_seconds / 3600)\n",
    "        return f\"About {hours} hour{'s' if hours > 1 else ''}\"\n",
    "\n",
    "\n",
    "\n",
    "async def get_document_processing_status(embedding_queue, document_id: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get detailed processing status for a specific document.\n",
    "\n",
    "    Args:\n",
    "        document_id (str): The unique identifier of the document.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: A dictionary with processing status or None if not found.\n",
    "    \"\"\"\n",
    "    # Check if document is actively being processed\n",
    "    is_in_progress = (document_id in embedding_queue.document_total_pages)\n",
    "    \n",
    "    if not is_in_progress:\n",
    "        # Check database for completed document\n",
    "        async with aiosqlite.connect(LOCAL_DB_PATH) as conn:\n",
    "            await conn._execute(conn._conn.enable_load_extension, True)\n",
    "            await conn._execute(sqlite_vec.load, conn._conn)\n",
    "\n",
    "            query = \"\"\"\n",
    "                SELECT \n",
    "                    d.total_pages,\n",
    "                    (SELECT COUNT(*) FROM page_images_vectors piv \n",
    "                     WHERE piv.page_id IN (\n",
    "                         SELECT pi.page_id FROM page_images pi \n",
    "                         WHERE pi.document_id = d.document_id\n",
    "                     )) as processed_pages\n",
    "                FROM documents d\n",
    "                WHERE d.document_id = ?\n",
    "            \"\"\"\n",
    "            async with conn.execute(query, (document_id,)) as cursor:\n",
    "                record = await cursor.fetchone()\n",
    "                if record:\n",
    "                    total_pages, processed_pages = record\n",
    "                    \n",
    "                    # A document is considered complete if it has processed pages \n",
    "                    # and has been fully processed through the pipeline\n",
    "                    is_complete = processed_pages > 0\n",
    "                    \n",
    "                    return {\n",
    "                        \"status\": \"completed\" if is_complete else \"not_found\",\n",
    "                        \"total_pages\": total_pages,\n",
    "                        \"processed_pages\": processed_pages,\n",
    "                        \"progress_percentage\": round((processed_pages / total_pages) * 100, 2)\n",
    "                    }\n",
    "            await conn._execute(conn._conn.enable_load_extension, False)\n",
    "        return None\n",
    "\n",
    "    # Document is currently being processed\n",
    "    total_pages = embedding_queue.document_total_pages[document_id]\n",
    "    processed_pages = embedding_queue.processed_pages_count.get(document_id, 0)\n",
    "    \n",
    "    # Calculate estimated time based on current processing rate\n",
    "    estimated_time = _calculate_estimated_time(embedding_queue, document_id, total_pages, processed_pages)\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"processing\",\n",
    "        \"total_pages\": total_pages,\n",
    "        \"processed_pages\": processed_pages,\n",
    "        \"progress_percentage\": round((processed_pages / total_pages) * 100, 2),\n",
    "        \"estimated_completion_time\": estimated_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b8d1c",
   "metadata": {},
   "source": [
    "### Functions to store PDF files in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59e2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _db_store_pdf_data(\n",
    "    conn: aiosqlite.Connection,\n",
    "    document_id: str,\n",
    "    title: str,\n",
    "    total_pages: int\n",
    ") -> None:\n",
    "    print(f\"Inputs to store pdf data: {document_id}, {title}, {total_pages}\")\n",
    "\n",
    "    query = \"\"\"\n",
    "        INSERT INTO documents (\n",
    "            document_id, title, total_pages\n",
    "        ) VALUES (?, ?, ?)\n",
    "    \"\"\"\n",
    "    await conn.execute(query, (\n",
    "        document_id, title, total_pages\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "async def _db_store_image_with_vector(\n",
    "    conn: aiosqlite.Connection,\n",
    "    document_id: str,\n",
    "    page_number: int,\n",
    "    page_text: str,\n",
    "    vector_data: Optional[List[float]] = None,\n",
    "    page_id: Optional[str] = None,\n",
    "    latex_code: Optional[str] = None\n",
    ") -> str:\n",
    "    print(f\"Vector data: {vector_data}\")\n",
    "\n",
    "    if page_id is None:\n",
    "        page_id = str(uuid4())\n",
    "\n",
    "    query = \"\"\"\n",
    "        INSERT INTO page_images (\n",
    "            page_id, document_id, page_number, page_text, latex_code\n",
    "        ) VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\"\n",
    "    await conn.execute(query, (page_id, document_id, page_number, page_text, latex_code))\n",
    "\n",
    "    if vector_data:\n",
    "        vector_query = \"\"\"\n",
    "            INSERT INTO page_images_vectors (page_id, vector_data)\n",
    "            VALUES (?, ?)\n",
    "        \"\"\"\n",
    "        await conn.execute(vector_query, (page_id, vector_data))\n",
    "\n",
    "    return page_id\n",
    "\n",
    "\n",
    "\n",
    "async def db_store_pdf_file(\n",
    "    document_id: str,\n",
    "    title: str,\n",
    "    page_texts: List[str],\n",
    "    vectors: Optional[List[List[float]]] = None,\n",
    "    page_ids: Optional[List[str]] = None,\n",
    "    latex_code: Optional[List[str]] = None,\n",
    ") -> None:\n",
    "    async with aiosqlite.connect(LOCAL_DB_PATH) as conn:\n",
    "        await conn.execute(\"BEGIN\")\n",
    "        try:\n",
    "            await _db_store_pdf_data(\n",
    "                conn, document_id, title, len(page_texts)\n",
    "            )\n",
    "            print(f\"Storing {len(page_texts)} images and vectors\")\n",
    "            print(f\"Vectors: {vectors}\")\n",
    "            for i, page_text in enumerate(page_texts):\n",
    "                page_vector = vectors[i] if vectors and i < len(vectors) else None\n",
    "                page_id = page_ids[i] if page_ids and i < len(page_ids) else None\n",
    "                await _db_store_image_with_vector(\n",
    "                    conn, document_id, i, page_text, page_vector, page_id, latex_code\n",
    "                )\n",
    "            await conn.commit()\n",
    "        except Exception as e:\n",
    "            await conn.rollback()\n",
    "            raise ValueError(f\"Failed to store PDF file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fee82",
   "metadata": {},
   "source": [
    "### Functions to store page vectors in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bee6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def store_page_vector(document_id: str, page_number: int, vector: list, page_id: Optional[str] = None) -> None:\n",
    "    \"\"\"Store page vector in database with page_id if provided\"\"\"\n",
    "    print(f\"[{datetime.now()}] Storing page vector in database - Document: {document_id}, Page: {page_number}\")\n",
    "    print(f\"SAMPLE VECTOR: {vector[:1]}\")  # Log first element for verification\n",
    "\n",
    "    # Ensure vector length matches the expected dimension (1536)\n",
    "    if len(vector) != 1536:\n",
    "        raise ValueError(f\"Vector length {len(vector)} does not match expected dimension 1536\")\n",
    "\n",
    "    # Convert vector to binary blob\n",
    "    vector_blob = struct.pack(f'<{len(vector)}f', *vector)\n",
    "\n",
    "    async with aiosqlite.connect(LOCAL_DB_PATH) as conn:\n",
    "        await conn._execute(conn._conn.enable_load_extension, True)\n",
    "        await conn._execute(sqlite_vec.load, conn._conn)\n",
    "\n",
    "        if page_id:\n",
    "            # Use page_id if provided\n",
    "            query = \"\"\"\n",
    "                INSERT OR REPLACE INTO page_images_vectors (page_id, vector_data) VALUES (?, ?)\n",
    "            \"\"\"\n",
    "            try:\n",
    "                await conn.execute(query, (page_id, vector_blob))\n",
    "                await conn.commit()\n",
    "                print(f\"[{datetime.now()}] Successfully stored page vector - Document: {document_id}, Page: {page_number}, Page ID: {page_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{datetime.now()}] Database error while storing page vector: {str(e)}\")\n",
    "                raise\n",
    "            finally:\n",
    "                await conn._execute(conn._conn.enable_load_extension, False)\n",
    "        else:\n",
    "            # Fall back to document_id and page_number\n",
    "            select_query = \"\"\"\n",
    "                SELECT page_id FROM page_images \n",
    "                WHERE document_id = ? AND page_number = ?\n",
    "            \"\"\"\n",
    "            async with conn.execute(select_query, (document_id, page_number)) as cursor:\n",
    "                row = await cursor.fetchone()\n",
    "                if row:\n",
    "                    page_id = row[0]\n",
    "                    query = \"\"\"\n",
    "                        INSERT OR REPLACE INTO page_images_vectors (page_id, vector_data) VALUES (?, ?)\n",
    "                    \"\"\"\n",
    "                    try:\n",
    "                        await conn.execute(query, (page_id, vector_blob))\n",
    "                        await conn.commit()\n",
    "                        print(f\"[{datetime.now()}] Successfully stored page vector - Document: {document_id}, Page: {page_number}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[{datetime.now()}] Database error while storing page vector: {str(e)}\")\n",
    "                        raise\n",
    "                    finally:\n",
    "                        await conn._execute(conn._conn.enable_load_extension, False)\n",
    "                else:\n",
    "                    print(f\"[{datetime.now()}] ERROR: Page not found for Document: {document_id}, Page: {page_number}\")\n",
    "                    await conn._execute(conn._conn.enable_load_extension, False)\n",
    "                    raise ValueError(\"Page not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141eb74",
   "metadata": {},
   "source": [
    "### Class embedding Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2089282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingQueue:\n",
    "    def __init__(self):\n",
    "        self.image_queue: Deque[Dict[str, Any]] = deque()\n",
    "        self.current_task = None\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.processing = False\n",
    "        self.processed_pages_count: Dict[str, int] = {}  # Track number of processed pages per document\n",
    "        self.document_total_pages: Dict[str, int] = {}\n",
    "        self.failed_pages: Dict[str, List[int]] = {}\n",
    "        self.document_start_times: Dict[str, float] = {}\n",
    "\n",
    "        print(f\"[{datetime.now()}] Initialized EmbeddingQueue instance\")\n",
    "\n",
    "\n",
    "    def start_background_tasks(self):\n",
    "        \"\"\"Start background tasks that require a running event loop\"\"\"\n",
    "        asyncio.create_task(self._check_stalled_documents_periodically())\n",
    "        print(f\"[{datetime.now()}] Started background task for checking stalled documents\")\n",
    "\n",
    "\n",
    "    async def _check_stalled_documents_periodically(self):\n",
    "        \"\"\"Periodically check for and process stalled documents\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                await asyncio.sleep(300)  # Check every 5 minutes\n",
    "                await self._check_stalled_documents()\n",
    "            except Exception as e:\n",
    "                print(f\"[{datetime.now()}] Error in stalled document checker: {str(e)}\")\n",
    "\n",
    "\n",
    "    def _start_stalled_checker(self):\n",
    "        \"\"\"Start a background task to periodically check for stalled documents\"\"\"\n",
    "        asyncio.create_task(self._check_stalled_documents_periodically())\n",
    "\n",
    "\n",
    "    async def _check_stalled_documents(self):\n",
    "        \"\"\"Check for documents that have stalled processing and compute their vectors\"\"\"\n",
    "        current_time = time.time()\n",
    "        documents_to_process = []\n",
    "        \n",
    "        print(f\"[{datetime.now()}] Checking for stalled documents...\")\n",
    "        \n",
    "        async with self.lock:\n",
    "            for doc_id in list(self.processed_pages_count.keys()):\n",
    "                # Skip documents that are already completed\n",
    "                if doc_id not in self.document_total_pages:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if this document has been processing for more than 10 minutes\n",
    "                if doc_id not in self.document_start_times:\n",
    "                    self.document_start_times[doc_id] = current_time\n",
    "                    continue\n",
    "                    \n",
    "                time_processing = current_time - self.document_start_times[doc_id]\n",
    "                if time_processing < 600:  # Less than 10 minutes\n",
    "                    continue\n",
    "                \n",
    "                # Calculate percentage processed\n",
    "                processed_count = self.processed_pages_count[doc_id]\n",
    "                total_expected = self.document_total_pages[doc_id]\n",
    "                percent_processed = (processed_count / total_expected) * 100\n",
    "                \n",
    "                # Get count of pages still in queue for this document\n",
    "                pages_in_queue = 0\n",
    "                for task in self.image_queue:\n",
    "                    if task['document_id'] == doc_id:\n",
    "                        pages_in_queue += 1\n",
    "                \n",
    "                # If no tasks are pending and we have at least 70% of pages, \n",
    "                # or if more than 20 minutes have passed and we have at least 50% of pages\n",
    "                stalled_timeout = (pages_in_queue == 0 and percent_processed >= 70) or \\\n",
    "                                (time_processing >= 1200 and percent_processed >= 50)\n",
    "                                \n",
    "                if stalled_timeout:\n",
    "                    print(f\"[{datetime.now()}] Document {doc_id} appears stalled:\")\n",
    "                    print(f\"- Processing time: {time_processing:.1f} seconds\")\n",
    "                    print(f\"- Processed {processed_count}/{total_expected} pages ({percent_processed:.1f}%)\")\n",
    "                    print(f\"- Pages still in queue: {pages_in_queue}\")\n",
    "                    \n",
    "                    # Document is considered complete if stalled conditions are met\n",
    "                    self._mark_document_complete(doc_id, processed_count, total_expected)\n",
    "        \n",
    "        # Process any stalled documents (outside of lock)\n",
    "        for doc_id in documents_to_process:\n",
    "            print(f\"[{datetime.now()}] Processing stalled document: {doc_id}\")\n",
    "            try:\n",
    "                await self._process_document_vector(doc_id)\n",
    "            except Exception as e:\n",
    "                print(f\"[{datetime.now()}] Error processing stalled document {doc_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "    async def add_image_task(self, document_id: str, document_title: str, image_bytes: bytes, page_number: int, total_pages: int, page_id: Optional[str] = None):\n",
    "        \"\"\"Add a new image embedding task to the queue\"\"\"\n",
    "        print(f\"[{datetime.now()}] Adding new task - Document ID: {document_id}, Page: {page_number+1}/{total_pages}\")\n",
    "\n",
    "        if document_id not in self.processed_pages_count:\n",
    "            self.processed_pages_count[document_id] = 0\n",
    "            self.document_total_pages[document_id] = total_pages\n",
    "            self.failed_pages[document_id] = []\n",
    "            self.document_start_times[document_id] = time.time()\n",
    "            print(f\"[{datetime.now()}] Initialized new document tracking - ID: {document_id}, Total Pages: {total_pages}\")\n",
    "\n",
    "        task = {\n",
    "            'document_id': document_id,\n",
    "            'document_title': document_title,\n",
    "            'image_bytes': image_bytes,\n",
    "            'page_number': page_number,\n",
    "            'page_id': page_id,\n",
    "            'timestamp': time.time(),\n",
    "            'retry_count': 0\n",
    "        }\n",
    "        self.image_queue.append(task)\n",
    "\n",
    "        queue_stats = {\n",
    "            'queue_size': len(self.image_queue),\n",
    "            'documents_in_progress': len(self.processed_pages_count),\n",
    "            'current_document': document_id,\n",
    "            'page_number': page_number\n",
    "        }\n",
    "        print(f\"[{datetime.now()}] Task added to queue. Queue stats: {json.dumps(queue_stats)}\")\n",
    "\n",
    "        if not self.processing:\n",
    "            print(f\"[{datetime.now()}] Queue processor not running. Starting new processing task.\")\n",
    "            asyncio.create_task(self.process_queue())\n",
    "\n",
    "\n",
    "    async def process_queue(self):\n",
    "        \"\"\"Process all tasks in the queue with improved error handling and retries\"\"\"\n",
    "        if self.processing:\n",
    "            print(f\"[{datetime.now()}] Queue processor already running\")\n",
    "            return\n",
    "\n",
    "        self.processing = True\n",
    "        print(f\"[{datetime.now()}] Starting queue processor\")\n",
    "\n",
    "        failed_tasks = []  # Track failed tasks for retry\n",
    "\n",
    "        while self.image_queue:\n",
    "            async with self.lock:\n",
    "                task = self.image_queue.popleft()\n",
    "                self.current_task = task\n",
    "\n",
    "            document_id = task['document_id']\n",
    "            page_number = task['page_number']\n",
    "            page_id = task.get('page_id')\n",
    "            wait_time = time.time() - task['timestamp']\n",
    "\n",
    "            print(f\"\"\"[{datetime.now()}] Starting task processing:\n",
    "                - Document ID: {document_id}\n",
    "                - Page: {page_number}\n",
    "                - Queue size: {len(self.image_queue)}\n",
    "                - Wait time: {wait_time:.2f} seconds\"\"\")\n",
    "\n",
    "            try:\n",
    "                processing_start = time.time()\n",
    "                files = {\n",
    "                    'files': ('image.png', task['image_bytes'], 'image/png')\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    print(f\"[{datetime.now()}] Calling embed_image API for document {document_id}, page {page_number}\")\n",
    "                    vector = await embed_images(files)  # This has built-in retry logic\n",
    "                    \n",
    "                    # Store vector in database\n",
    "                    await store_page_vector(document_id, page_number, vector, page_id)\n",
    "                    \n",
    "                    # Update processed page count\n",
    "                    self.processed_pages_count[document_id] = self.processed_pages_count.get(document_id, 0) + 1\n",
    "                    \n",
    "                    # Check if document is complete\n",
    "                    await self._check_document_completion(document_id)\n",
    "                    \n",
    "                    processing_time = time.time() - processing_start\n",
    "\n",
    "                    print(f\"\"\"[{datetime.now()}] Task completed successfully:\n",
    "                        - Document ID: {document_id}\n",
    "                        - Page: {page_number}\n",
    "                        - Processing time: {processing_time:.2f} seconds\n",
    "                        - Vector size: {len(vector)}\"\"\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[{datetime.now()}] Error embedding image for document {document_id}, page {page_number}: {str(e)}\")\n",
    "                    \n",
    "                    # Track this failed page\n",
    "                    if document_id in self.failed_pages:\n",
    "                        self.failed_pages[document_id].append(page_number)\n",
    "                    \n",
    "                    # Add to failed tasks queue for later retry if under retry limit\n",
    "                    retry_count = task.get('retry_count', 0) + 1\n",
    "                    if retry_count <= 3:  # Limit retries to 3 attempts\n",
    "                        task['retry_count'] = retry_count\n",
    "                        task['last_error'] = str(e)\n",
    "                        task['last_attempt'] = time.time()\n",
    "                        failed_tasks.append(task)\n",
    "                        print(f\"[{datetime.now()}] Added to retry queue (attempt {retry_count}/3)\")\n",
    "                    else:\n",
    "                        print(f\"[{datetime.now()}] Max retries exceeded for document {document_id}, page {page_number}\")\n",
    "                        # Check if we should compute document vector despite this failure\n",
    "                        await self._check_document_completion(document_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{datetime.now()}] Error processing task for document {document_id}, page {page_number}: {str(e)}\")\n",
    "            finally:\n",
    "                self.current_task = None\n",
    "\n",
    "        # Process failed tasks if any\n",
    "        if failed_tasks:\n",
    "            print(f\"[{datetime.now()}] Processing {len(failed_tasks)} failed tasks after a delay\")\n",
    "            # Wait a bit before retrying\n",
    "            await asyncio.sleep(10)\n",
    "            # Add failed tasks back to the queue\n",
    "            async with self.lock:\n",
    "                for task in failed_tasks:\n",
    "                    self.image_queue.append(task)\n",
    "            # Process the queue again\n",
    "            return await self.process_queue()\n",
    "            \n",
    "        self.processing = False\n",
    "        print(f\"[{datetime.now()}] Queue processor finished - no more tasks in queue\")\n",
    "\n",
    "\n",
    "    async def _check_document_completion(self, document_id: str):\n",
    "        \"\"\"Check if a document's page processing is complete\"\"\"\n",
    "        if document_id not in self.processed_pages_count or document_id not in self.document_total_pages:\n",
    "            return\n",
    "            \n",
    "        total_pages = self.document_total_pages[document_id]\n",
    "        processed_pages = self.processed_pages_count[document_id]\n",
    "        \n",
    "        # Calculate what percentage of pages are done\n",
    "        percent_complete = (processed_pages / total_pages) * 100\n",
    "        \n",
    "        # Count pages still in queue for this document\n",
    "        pages_in_queue = 0\n",
    "        for task in self.image_queue:\n",
    "            if task['document_id'] == document_id:\n",
    "                pages_in_queue += 1\n",
    "        \n",
    "        # Document is considered complete if either:\n",
    "        # 1. All pages processed successfully (100%)\n",
    "        # 2. No more pages in queue AND at least 70% processed\n",
    "        # 3. At least 95% processed (regardless of queue)\n",
    "        is_complete = (\n",
    "            (processed_pages == total_pages) or\n",
    "            (pages_in_queue == 0 and percent_complete >= 70) or\n",
    "            (percent_complete >= 95)\n",
    "        )\n",
    "        \n",
    "        if is_complete:\n",
    "            print(f\"[{datetime.now()}] Document processing complete: {document_id} with {processed_pages}/{total_pages} pages ({percent_complete:.1f}%)\")\n",
    "            self._mark_document_complete(document_id, processed_pages, total_pages)\n",
    "\n",
    "\n",
    "    def _mark_document_complete(self, document_id: str, processed_pages: int, total_pages: int):\n",
    "        \"\"\"Mark a document as complete and clean up tracking data\"\"\"\n",
    "        print(f\"[{datetime.now()}] Marking document as complete: {document_id}\")\n",
    "        print(f\"[{datetime.now()}] - Processed pages: {processed_pages}/{total_pages}\")\n",
    "        \n",
    "        # Clean up tracking data\n",
    "        if document_id in self.processed_pages_count:\n",
    "            del self.processed_pages_count[document_id]\n",
    "        if document_id in self.document_total_pages:\n",
    "            del self.document_total_pages[document_id]\n",
    "        if document_id in self.failed_pages:\n",
    "            del self.failed_pages[document_id]\n",
    "        if document_id in self.document_start_times:\n",
    "            del self.document_start_times[document_id]\n",
    "\n",
    "\n",
    "    async def pause_current_task(self):\n",
    "        \"\"\"Pause the current task and return it to the queue\"\"\"\n",
    "        async with self.lock:\n",
    "            if self.current_task:\n",
    "                print(f\"[{datetime.now()}] Pausing current task and returning to queue\")\n",
    "                self.image_queue.appendleft(self.current_task)\n",
    "                self.current_task = None\n",
    "            self.processing = False\n",
    "            print(f\"[{datetime.now()}] Queue processing paused\")\n",
    "\n",
    "\n",
    "    async def resume_processing(self):\n",
    "        \"\"\"Resume queue processing if paused\"\"\"\n",
    "        async with self.lock:\n",
    "            if not self.processing and self.image_queue:\n",
    "                print(f\"[{datetime.now()}] Resuming queue processing\")\n",
    "                self.processing = True\n",
    "                try:\n",
    "                    await self.process_queue()\n",
    "                except Exception as e:\n",
    "                    print(f\"[{datetime.now()}] Error during queue processing: {str(e)}\")\n",
    "                    self.processing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99b335",
   "metadata": {},
   "source": [
    "#### Start it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30637747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-25 18:47:38.798833] Initializing global embedding queue\n",
      "[2025-05-25 18:47:38.798977] Initialized EmbeddingQueue instance\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] Initializing global embedding queue\")\n",
    "embedding_queue = EmbeddingQueue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c838e49",
   "metadata": {},
   "source": [
    "### Run the filling process (around 45 seconds for 10 rows of the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c2bb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-25 18:47:40.875511] Started background task for checking stalled documents\n",
      "\n",
      "Processing 0 PDF files, 10 image files from folder: extracted_data\n",
      "\n",
      "Queuing image: sample_9.jpg\n",
      "\n",
      "Queuing image: sample_8.jpg\n",
      "\n",
      "Queuing image: sample_5.jpg\n",
      "\n",
      "Queuing image: sample_4.jpg\n",
      "\n",
      "Queuing image: sample_6.jpg\n",
      "\n",
      "Queuing image: sample_7.jpg\n",
      "\n",
      "Queuing image: sample_3.jpg\n",
      "\n",
      "Queuing image: sample_2.jpg\n",
      "\n",
      "Queuing image: sample_0.jpg\n",
      "\n",
      "Queuing image: sample_1.jpg\n",
      "Processing image extracted_data/sample_9.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_9.txt\n",
      "Extracted image as a single-page document: sample_9.jpg\n",
      "Processing image extracted_data/sample_8.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_8.txt\n",
      "Extracted image as a single-page document: sample_8.jpg\n",
      "File saved locally: processed_data/402edd1c-2a77-4a30-87a1-7bdc2ef31465.jpg\n",
      "File saved locally: processed_data/91750c08-d0b0-4b0a-8f40-148c5ffb4ac5.jpg\n",
      "Inputs to store pdf data: 402edd1c-2a77-4a30-87a1-7bdc2ef31465, sample_9.jpg, 1\n",
      "Inputs to store pdf data: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5, sample_8.jpg, 1\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/0d60403e-a099-4bbe-9901-704315dbdc93_full.jpg\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/0d60403e-a099-4bbe-9901-704315dbdc93_thumb.jpg\n",
      "[2025-05-25 18:47:40.955582] Adding new task - Document ID: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5, Page: 1/1\n",
      "[2025-05-25 18:47:40.955605] Initialized new document tracking - ID: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5, Total Pages: 1\n",
      "[2025-05-25 18:47:40.955611] Task added to queue. Queue stats: {\"queue_size\": 1, \"documents_in_progress\": 1, \"current_document\": \"91750c08-d0b0-4b0a-8f40-148c5ffb4ac5\", \"page_number\": 0}\n",
      "[2025-05-25 18:47:40.955622] Queue processor not running. Starting new processing task.\n",
      "Successfully queued image sample_8.jpg for processing\n",
      "[2025-05-25 18:47:40.955669] Starting queue processor\n",
      "[2025-05-25 18:47:40.955679] Starting task processing:\n",
      "                - Document ID: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5\n",
      "                - Page: 0\n",
      "                - Queue size: 0\n",
      "                - Wait time: 0.00 seconds\n",
      "[2025-05-25 18:47:40.955684] Calling embed_image API for document 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpwabmop7r.jpg\n",
      "[2025-05-25 18:47:43.744763] Storing page vector in database - Document: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5, Page: 0\n",
      "SAMPLE VECTOR: [0.012763180769979954]\n",
      "Starting monitoring for document 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5 (sample_8.jpg)\n",
      "Processing image extracted_data/sample_5.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_5.txt\n",
      "Extracted image as a single-page document: sample_5.jpg\n",
      "File saved locally: processed_data/85a42915-b0fe-4038-bd74-4831c25c306a.jpg\n",
      "File saved locally: processed_data/82b3e859-e3e6-4986-bc7a-1687747d628c_full.jpg\n",
      "Inputs to store pdf data: 85a42915-b0fe-4038-bd74-4831c25c306a, sample_5.jpg, 1\n",
      "File saved locally: processed_data/82b3e859-e3e6-4986-bc7a-1687747d628c_thumb.jpg\n",
      "[2025-05-25 18:47:43.826656] Adding new task - Document ID: 402edd1c-2a77-4a30-87a1-7bdc2ef31465, Page: 1/1\n",
      "[2025-05-25 18:47:43.826679] Initialized new document tracking - ID: 402edd1c-2a77-4a30-87a1-7bdc2ef31465, Total Pages: 1\n",
      "[2025-05-25 18:47:43.826684] Task added to queue. Queue stats: {\"queue_size\": 1, \"documents_in_progress\": 2, \"current_document\": \"402edd1c-2a77-4a30-87a1-7bdc2ef31465\", \"page_number\": 0}\n",
      "Successfully queued image sample_9.jpg for processing\n",
      "Starting monitoring for document 402edd1c-2a77-4a30-87a1-7bdc2ef31465 (sample_9.jpg)\n",
      "Processing image extracted_data/sample_4.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_4.txt\n",
      "Extracted image as a single-page document: sample_4.jpg\n",
      "Successfully queued sample_9.jpg\n",
      "[2025-05-25 18:47:43.827632] Successfully stored page vector - Document: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5, Page: 0, Page ID: 0d60403e-a099-4bbe-9901-704315dbdc93\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/4bf929d2-e473-4bf3-9afd-998dc631888e.jpg\n",
      "Inputs to store pdf data: 4bf929d2-e473-4bf3-9afd-998dc631888e, sample_4.jpg, 1\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/3810b890-f376-406b-9b26-fe32ff40ebab_full.jpg\n",
      "[2025-05-25 18:47:43.839589] Document processing complete: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5 with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:43.839610] Marking document as complete: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5\n",
      "[2025-05-25 18:47:43.839613] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:43.839617] Task completed successfully:\n",
      "                        - Document ID: 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.88 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:43.839627] Starting task processing:\n",
      "                - Document ID: 402edd1c-2a77-4a30-87a1-7bdc2ef31465\n",
      "                - Page: 0\n",
      "                - Queue size: 0\n",
      "                - Wait time: 0.01 seconds\n",
      "[2025-05-25 18:47:43.839631] Calling embed_image API for document 402edd1c-2a77-4a30-87a1-7bdc2ef31465, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmphuoqtvq9.jpg\n",
      "[2025-05-25 18:47:46.295276] Storing page vector in database - Document: 402edd1c-2a77-4a30-87a1-7bdc2ef31465, Page: 0\n",
      "SAMPLE VECTOR: [0.012763180769979954]\n",
      "File saved locally: processed_data/3810b890-f376-406b-9b26-fe32ff40ebab_thumb.jpg\n",
      "[2025-05-25 18:47:46.352653] Adding new task - Document ID: 85a42915-b0fe-4038-bd74-4831c25c306a, Page: 1/1\n",
      "[2025-05-25 18:47:46.352679] Initialized new document tracking - ID: 85a42915-b0fe-4038-bd74-4831c25c306a, Total Pages: 1\n",
      "[2025-05-25 18:47:46.352685] Task added to queue. Queue stats: {\"queue_size\": 1, \"documents_in_progress\": 2, \"current_document\": \"85a42915-b0fe-4038-bd74-4831c25c306a\", \"page_number\": 0}\n",
      "Successfully queued image sample_5.jpg for processing\n",
      "Starting monitoring for document 85a42915-b0fe-4038-bd74-4831c25c306a (sample_5.jpg)\n",
      "Processing image extracted_data/sample_6.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_6.txt\n",
      "Extracted image as a single-page document: sample_6.jpg\n",
      "File saved locally: processed_data/e44ebe43-bbf5-426c-a4ee-edcaf95b81ca.jpg\n",
      "File saved locally: processed_data/934e1451-ebf1-4777-8c95-d5c223ad2dd6_full.jpg\n",
      "Inputs to store pdf data: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca, sample_6.jpg, 1\n",
      "File saved locally: processed_data/934e1451-ebf1-4777-8c95-d5c223ad2dd6_thumb.jpg\n",
      "[2025-05-25 18:47:46.411406] Adding new task - Document ID: 4bf929d2-e473-4bf3-9afd-998dc631888e, Page: 1/1\n",
      "[2025-05-25 18:47:46.411430] Initialized new document tracking - ID: 4bf929d2-e473-4bf3-9afd-998dc631888e, Total Pages: 1\n",
      "[2025-05-25 18:47:46.411434] Task added to queue. Queue stats: {\"queue_size\": 2, \"documents_in_progress\": 3, \"current_document\": \"4bf929d2-e473-4bf3-9afd-998dc631888e\", \"page_number\": 0}\n",
      "Successfully queued image sample_4.jpg for processing\n",
      "Starting monitoring for document 4bf929d2-e473-4bf3-9afd-998dc631888e (sample_4.jpg)\n",
      "Processing image extracted_data/sample_7.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_7.txt\n",
      "Extracted image as a single-page document: sample_7.jpg\n",
      "[2025-05-25 18:47:46.412279] Successfully stored page vector - Document: 402edd1c-2a77-4a30-87a1-7bdc2ef31465, Page: 0, Page ID: 82b3e859-e3e6-4986-bc7a-1687747d628c\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f.jpg\n",
      "Inputs to store pdf data: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f, sample_7.jpg, 1\n",
      "File saved locally: processed_data/ea4f0370-98be-4519-a32e-4aa1b26ad0f2_full.jpg\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "[2025-05-25 18:47:46.423551] Document processing complete: 402edd1c-2a77-4a30-87a1-7bdc2ef31465 with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:46.423563] Marking document as complete: 402edd1c-2a77-4a30-87a1-7bdc2ef31465\n",
      "[2025-05-25 18:47:46.423566] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:46.423570] Task completed successfully:\n",
      "                        - Document ID: 402edd1c-2a77-4a30-87a1-7bdc2ef31465\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.58 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:46.423578] Starting task processing:\n",
      "                - Document ID: 85a42915-b0fe-4038-bd74-4831c25c306a\n",
      "                - Page: 0\n",
      "                - Queue size: 1\n",
      "                - Wait time: 0.07 seconds\n",
      "[2025-05-25 18:47:46.423582] Calling embed_image API for document 85a42915-b0fe-4038-bd74-4831c25c306a, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmphvu3vaqz.jpg\n",
      "[2025-05-25 18:47:48.931850] Storing page vector in database - Document: 85a42915-b0fe-4038-bd74-4831c25c306a, Page: 0\n",
      "SAMPLE VECTOR: [0.0360603891313076]\n",
      "File saved locally: processed_data/ea4f0370-98be-4519-a32e-4aa1b26ad0f2_thumb.jpg\n",
      "[2025-05-25 18:47:48.972841] Adding new task - Document ID: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca, Page: 1/1\n",
      "[2025-05-25 18:47:48.972863] Initialized new document tracking - ID: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca, Total Pages: 1\n",
      "[2025-05-25 18:47:48.972869] Task added to queue. Queue stats: {\"queue_size\": 2, \"documents_in_progress\": 3, \"current_document\": \"e44ebe43-bbf5-426c-a4ee-edcaf95b81ca\", \"page_number\": 0}\n",
      "Successfully queued image sample_6.jpg for processing\n",
      "Starting monitoring for document e44ebe43-bbf5-426c-a4ee-edcaf95b81ca (sample_6.jpg)\n",
      "Processing image extracted_data/sample_3.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_3.txt\n",
      "Extracted image as a single-page document: sample_3.jpg\n",
      "File saved locally: processed_data/203d78a6-c706-4d68-9be5-d90bfd1890d3.jpg\n",
      "Inputs to store pdf data: 203d78a6-c706-4d68-9be5-d90bfd1890d3, sample_3.jpg, 1\n",
      "File saved locally: processed_data/39e0fbde-1f41-4595-a473-8793ab46b609_full.jpg\n",
      "[2025-05-25 18:47:48.988797] Successfully stored page vector - Document: 85a42915-b0fe-4038-bd74-4831c25c306a, Page: 0, Page ID: 3810b890-f376-406b-9b26-fe32ff40ebab\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/39e0fbde-1f41-4595-a473-8793ab46b609_thumb.jpg\n",
      "[2025-05-25 18:47:49.027655] Adding new task - Document ID: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f, Page: 1/1\n",
      "[2025-05-25 18:47:49.027680] Initialized new document tracking - ID: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f, Total Pages: 1\n",
      "[2025-05-25 18:47:49.027686] Task added to queue. Queue stats: {\"queue_size\": 3, \"documents_in_progress\": 4, \"current_document\": \"89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f\", \"page_number\": 0}\n",
      "Successfully queued image sample_7.jpg for processing\n",
      "Starting monitoring for document 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f (sample_7.jpg)\n",
      "Processing image extracted_data/sample_2.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_2.txt\n",
      "Extracted image as a single-page document: sample_2.jpg\n",
      "[2025-05-25 18:47:49.028716] Document processing complete: 85a42915-b0fe-4038-bd74-4831c25c306a with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:49.028725] Marking document as complete: 85a42915-b0fe-4038-bd74-4831c25c306a\n",
      "[2025-05-25 18:47:49.028728] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:49.028731] Task completed successfully:\n",
      "                        - Document ID: 85a42915-b0fe-4038-bd74-4831c25c306a\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.61 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:49.028738] Starting task processing:\n",
      "                - Document ID: 4bf929d2-e473-4bf3-9afd-998dc631888e\n",
      "                - Page: 0\n",
      "                - Queue size: 2\n",
      "                - Wait time: 2.62 seconds\n",
      "[2025-05-25 18:47:49.028742] Calling embed_image API for document 4bf929d2-e473-4bf3-9afd-998dc631888e, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpuu0ydfg9.jpg\n",
      "[2025-05-25 18:47:51.334471] Storing page vector in database - Document: 4bf929d2-e473-4bf3-9afd-998dc631888e, Page: 0\n",
      "SAMPLE VECTOR: [0.024122070521116257]\n",
      "File saved locally: processed_data/e2049073-6dee-4ad0-bb9a-3731b0a0e28e.jpg\n",
      "Inputs to store pdf data: e2049073-6dee-4ad0-bb9a-3731b0a0e28e, sample_2.jpg, 1\n",
      "File saved locally: processed_data/8c4a1634-f167-43ac-9469-a8d50aecfcea_full.jpg\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "File saved locally: processed_data/8c4a1634-f167-43ac-9469-a8d50aecfcea_thumb.jpg\n",
      "[2025-05-25 18:47:51.390341] Adding new task - Document ID: 203d78a6-c706-4d68-9be5-d90bfd1890d3, Page: 1/1\n",
      "[2025-05-25 18:47:51.390366] Initialized new document tracking - ID: 203d78a6-c706-4d68-9be5-d90bfd1890d3, Total Pages: 1\n",
      "[2025-05-25 18:47:51.390371] Task added to queue. Queue stats: {\"queue_size\": 3, \"documents_in_progress\": 4, \"current_document\": \"203d78a6-c706-4d68-9be5-d90bfd1890d3\", \"page_number\": 0}\n",
      "Successfully queued image sample_3.jpg for processing\n",
      "Starting monitoring for document 203d78a6-c706-4d68-9be5-d90bfd1890d3 (sample_3.jpg)\n",
      "Processing image extracted_data/sample_0.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_0.txt\n",
      "Extracted image as a single-page document: sample_0.jpg\n",
      "File saved locally: processed_data/76e2478e-d44f-423e-960f-01c03a43752e.jpg\n",
      "[2025-05-25 18:47:51.392323] Successfully stored page vector - Document: 4bf929d2-e473-4bf3-9afd-998dc631888e, Page: 0, Page ID: 934e1451-ebf1-4777-8c95-d5c223ad2dd6\n",
      "Inputs to store pdf data: 76e2478e-d44f-423e-960f-01c03a43752e, sample_0.jpg, 1\n",
      "File saved locally: processed_data/4f8d850f-5d34-4392-b2c2-c2442a0e2aa6_full.jpg\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "[2025-05-25 18:47:51.407236] Document processing complete: 4bf929d2-e473-4bf3-9afd-998dc631888e with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:51.407248] Marking document as complete: 4bf929d2-e473-4bf3-9afd-998dc631888e\n",
      "[2025-05-25 18:47:51.407251] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:51.407255] Task completed successfully:\n",
      "                        - Document ID: 4bf929d2-e473-4bf3-9afd-998dc631888e\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.38 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:51.407261] Starting task processing:\n",
      "                - Document ID: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca\n",
      "                - Page: 0\n",
      "                - Queue size: 2\n",
      "                - Wait time: 2.43 seconds\n",
      "[2025-05-25 18:47:51.408016] Calling embed_image API for document e44ebe43-bbf5-426c-a4ee-edcaf95b81ca, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpfpzo3r4p.jpg\n",
      "[2025-05-25 18:47:53.758966] Storing page vector in database - Document: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca, Page: 0\n",
      "SAMPLE VECTOR: [0.031977951526641846]\n",
      "File saved locally: processed_data/4f8d850f-5d34-4392-b2c2-c2442a0e2aa6_thumb.jpg\n",
      "[2025-05-25 18:47:53.810872] Adding new task - Document ID: e2049073-6dee-4ad0-bb9a-3731b0a0e28e, Page: 1/1\n",
      "[2025-05-25 18:47:53.810894] Initialized new document tracking - ID: e2049073-6dee-4ad0-bb9a-3731b0a0e28e, Total Pages: 1\n",
      "[2025-05-25 18:47:53.810899] Task added to queue. Queue stats: {\"queue_size\": 3, \"documents_in_progress\": 4, \"current_document\": \"e2049073-6dee-4ad0-bb9a-3731b0a0e28e\", \"page_number\": 0}\n",
      "Successfully queued image sample_2.jpg for processing\n",
      "Starting monitoring for document e2049073-6dee-4ad0-bb9a-3731b0a0e28e (sample_2.jpg)\n",
      "Processing image extracted_data/sample_1.jpg\n",
      "Looking for LaTeX file: extracted_data/sample_1.txt\n",
      "Extracted image as a single-page document: sample_1.jpg\n",
      "File saved locally: processed_data/b93f38fe-8296-44da-a678-6ab125b4f009.jpg\n",
      "File saved locally: processed_data/05e6004b-9db1-4e0d-98a7-0fdaf15c3009_full.jpg\n",
      "Inputs to store pdf data: b93f38fe-8296-44da-a678-6ab125b4f009, sample_1.jpg, 1\n",
      "[2025-05-25 18:47:53.827183] Successfully stored page vector - Document: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca, Page: 0, Page ID: ea4f0370-98be-4519-a32e-4aa1b26ad0f2\n",
      "File saved locally: processed_data/05e6004b-9db1-4e0d-98a7-0fdaf15c3009_thumb.jpg\n",
      "[2025-05-25 18:47:53.879087] Adding new task - Document ID: 76e2478e-d44f-423e-960f-01c03a43752e, Page: 1/1\n",
      "[2025-05-25 18:47:53.879108] Initialized new document tracking - ID: 76e2478e-d44f-423e-960f-01c03a43752e, Total Pages: 1\n",
      "[2025-05-25 18:47:53.879113] Task added to queue. Queue stats: {\"queue_size\": 4, \"documents_in_progress\": 5, \"current_document\": \"76e2478e-d44f-423e-960f-01c03a43752e\", \"page_number\": 0}\n",
      "Successfully queued image sample_0.jpg for processing\n",
      "Starting monitoring for document 76e2478e-d44f-423e-960f-01c03a43752e (sample_0.jpg)\n",
      "Storing 1 images and vectors\n",
      "Vectors: None\n",
      "Vector data: None\n",
      "[2025-05-25 18:47:53.879228] Document processing complete: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:53.879234] Marking document as complete: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca\n",
      "[2025-05-25 18:47:53.879237] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:53.879241] Task completed successfully:\n",
      "                        - Document ID: e44ebe43-bbf5-426c-a4ee-edcaf95b81ca\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.47 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:53.879248] Starting task processing:\n",
      "                - Document ID: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f\n",
      "                - Page: 0\n",
      "                - Queue size: 3\n",
      "                - Wait time: 4.85 seconds\n",
      "[2025-05-25 18:47:53.879833] Calling embed_image API for document 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpxlc4t5qb.jpg\n",
      "[2025-05-25 18:47:56.227534] Storing page vector in database - Document: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f, Page: 0\n",
      "SAMPLE VECTOR: [0.06259284913539886]\n",
      "File saved locally: processed_data/315fd4b8-6e87-4e66-8849-1457e2468b57_full.jpg\n",
      "[2025-05-25 18:47:56.242918] Successfully stored page vector - Document: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f, Page: 0, Page ID: 39e0fbde-1f41-4595-a473-8793ab46b609\n",
      "File saved locally: processed_data/315fd4b8-6e87-4e66-8849-1457e2468b57_thumb.jpg\n",
      "[2025-05-25 18:47:56.292906] Adding new task - Document ID: b93f38fe-8296-44da-a678-6ab125b4f009, Page: 1/1\n",
      "[2025-05-25 18:47:56.292924] Initialized new document tracking - ID: b93f38fe-8296-44da-a678-6ab125b4f009, Total Pages: 1\n",
      "[2025-05-25 18:47:56.292928] Task added to queue. Queue stats: {\"queue_size\": 4, \"documents_in_progress\": 5, \"current_document\": \"b93f38fe-8296-44da-a678-6ab125b4f009\", \"page_number\": 0}\n",
      "Successfully queued image sample_1.jpg for processing\n",
      "Starting monitoring for document b93f38fe-8296-44da-a678-6ab125b4f009 (sample_1.jpg)\n",
      "[2025-05-25 18:47:56.293020] Document processing complete: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:56.293028] Marking document as complete: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f\n",
      "[2025-05-25 18:47:56.293030] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:56.293034] Task completed successfully:\n",
      "                        - Document ID: 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.41 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:56.293041] Starting task processing:\n",
      "                - Document ID: 203d78a6-c706-4d68-9be5-d90bfd1890d3\n",
      "                - Page: 0\n",
      "                - Queue size: 3\n",
      "                - Wait time: 4.90 seconds\n",
      "[2025-05-25 18:47:56.293045] Calling embed_image API for document 203d78a6-c706-4d68-9be5-d90bfd1890d3, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpx_kbh7kt.jpg\n",
      "[2025-05-25 18:47:58.670822] Storing page vector in database - Document: 203d78a6-c706-4d68-9be5-d90bfd1890d3, Page: 0\n",
      "SAMPLE VECTOR: [-0.0117929233238101]\n",
      "[2025-05-25 18:47:58.676509] Successfully stored page vector - Document: 203d78a6-c706-4d68-9be5-d90bfd1890d3, Page: 0, Page ID: 8c4a1634-f167-43ac-9469-a8d50aecfcea\n",
      "[2025-05-25 18:47:58.680760] Document processing complete: 203d78a6-c706-4d68-9be5-d90bfd1890d3 with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:47:58.680778] Marking document as complete: 203d78a6-c706-4d68-9be5-d90bfd1890d3\n",
      "[2025-05-25 18:47:58.680782] - Processed pages: 1/1\n",
      "[2025-05-25 18:47:58.680787] Task completed successfully:\n",
      "                        - Document ID: 203d78a6-c706-4d68-9be5-d90bfd1890d3\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.39 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:47:58.680799] Starting task processing:\n",
      "                - Document ID: e2049073-6dee-4ad0-bb9a-3731b0a0e28e\n",
      "                - Page: 0\n",
      "                - Queue size: 2\n",
      "                - Wait time: 4.87 seconds\n",
      "[2025-05-25 18:47:58.680803] Calling embed_image API for document e2049073-6dee-4ad0-bb9a-3731b0a0e28e, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpw2zxuhri.jpg\n",
      "[2025-05-25 18:48:01.103681] Storing page vector in database - Document: e2049073-6dee-4ad0-bb9a-3731b0a0e28e, Page: 0\n",
      "SAMPLE VECTOR: [-0.02403980866074562]\n",
      "[2025-05-25 18:48:01.109404] Successfully stored page vector - Document: e2049073-6dee-4ad0-bb9a-3731b0a0e28e, Page: 0, Page ID: 4f8d850f-5d34-4392-b2c2-c2442a0e2aa6\n",
      "[2025-05-25 18:48:01.113773] Document processing complete: e2049073-6dee-4ad0-bb9a-3731b0a0e28e with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:48:01.113787] Marking document as complete: e2049073-6dee-4ad0-bb9a-3731b0a0e28e\n",
      "[2025-05-25 18:48:01.113790] - Processed pages: 1/1\n",
      "[2025-05-25 18:48:01.113794] Task completed successfully:\n",
      "                        - Document ID: e2049073-6dee-4ad0-bb9a-3731b0a0e28e\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.43 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:48:01.113801] Starting task processing:\n",
      "                - Document ID: 76e2478e-d44f-423e-960f-01c03a43752e\n",
      "                - Page: 0\n",
      "                - Queue size: 1\n",
      "                - Wait time: 7.23 seconds\n",
      "[2025-05-25 18:48:01.113805] Calling embed_image API for document 76e2478e-d44f-423e-960f-01c03a43752e, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpfvi0kol_.jpg\n",
      "[2025-05-25 18:48:03.514409] Storing page vector in database - Document: 76e2478e-d44f-423e-960f-01c03a43752e, Page: 0\n",
      "SAMPLE VECTOR: [0.011937075294554234]\n",
      "[2025-05-25 18:48:03.520330] Successfully stored page vector - Document: 76e2478e-d44f-423e-960f-01c03a43752e, Page: 0, Page ID: 05e6004b-9db1-4e0d-98a7-0fdaf15c3009\n",
      "[2025-05-25 18:48:03.525517] Document processing complete: 76e2478e-d44f-423e-960f-01c03a43752e with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:48:03.525535] Marking document as complete: 76e2478e-d44f-423e-960f-01c03a43752e\n",
      "[2025-05-25 18:48:03.525539] - Processed pages: 1/1\n",
      "[2025-05-25 18:48:03.525543] Task completed successfully:\n",
      "                        - Document ID: 76e2478e-d44f-423e-960f-01c03a43752e\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.41 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:48:03.525551] Starting task processing:\n",
      "                - Document ID: b93f38fe-8296-44da-a678-6ab125b4f009\n",
      "                - Page: 0\n",
      "                - Queue size: 0\n",
      "                - Wait time: 7.23 seconds\n",
      "[2025-05-25 18:48:03.525555] Calling embed_image API for document b93f38fe-8296-44da-a678-6ab125b4f009, page 0\n",
      "Processing image from temporary file: /var/folders/9_/6_pftw5j2fs59x1_72k986500000gn/T/tmpv2b8qui8.jpg\n",
      "[2025-05-25 18:48:05.840152] Storing page vector in database - Document: b93f38fe-8296-44da-a678-6ab125b4f009, Page: 0\n",
      "SAMPLE VECTOR: [0.021377941593527794]\n",
      "[2025-05-25 18:48:05.846090] Successfully stored page vector - Document: b93f38fe-8296-44da-a678-6ab125b4f009, Page: 0, Page ID: 315fd4b8-6e87-4e66-8849-1457e2468b57\n",
      "[2025-05-25 18:48:05.850648] Document processing complete: b93f38fe-8296-44da-a678-6ab125b4f009 with 1/1 pages (100.0%)\n",
      "[2025-05-25 18:48:05.850665] Marking document as complete: b93f38fe-8296-44da-a678-6ab125b4f009\n",
      "[2025-05-25 18:48:05.850668] - Processed pages: 1/1\n",
      "[2025-05-25 18:48:05.850672] Task completed successfully:\n",
      "                        - Document ID: b93f38fe-8296-44da-a678-6ab125b4f009\n",
      "                        - Page: 0\n",
      "                        - Processing time: 2.33 seconds\n",
      "                        - Vector size: 1536\n",
      "[2025-05-25 18:48:05.850676] Queue processor finished - no more tasks in queue\n",
      "Document 91750c08-d0b0-4b0a-8f40-148c5ffb4ac5 (sample_8.jpg) processing completed successfully.\n",
      "Document 402edd1c-2a77-4a30-87a1-7bdc2ef31465 (sample_9.jpg) processing completed successfully.\n",
      "Successfully processed sample_9.jpg\n",
      "Successfully queued sample_8.jpg\n",
      "Successfully processed sample_8.jpg\n",
      "Successfully queued sample_5.jpg\n",
      "Document 85a42915-b0fe-4038-bd74-4831c25c306a (sample_5.jpg) processing completed successfully.\n",
      "Successfully processed sample_5.jpg\n",
      "Successfully queued sample_4.jpg\n",
      "Document 4bf929d2-e473-4bf3-9afd-998dc631888e (sample_4.jpg) processing completed successfully.\n",
      "Successfully processed sample_4.jpg\n",
      "Successfully queued sample_6.jpg\n",
      "Document e44ebe43-bbf5-426c-a4ee-edcaf95b81ca (sample_6.jpg) processing completed successfully.\n",
      "Successfully processed sample_6.jpg\n",
      "Successfully queued sample_7.jpg\n",
      "Document 89b4ae85-6c9e-4d61-8ab9-10ea18b33f5f (sample_7.jpg) processing completed successfully.\n",
      "Successfully processed sample_7.jpg\n",
      "Successfully queued sample_3.jpg\n",
      "Document 203d78a6-c706-4d68-9be5-d90bfd1890d3 (sample_3.jpg) processing completed successfully.\n",
      "Successfully processed sample_3.jpg\n",
      "Successfully queued sample_2.jpg\n",
      "Document e2049073-6dee-4ad0-bb9a-3731b0a0e28e (sample_2.jpg) processing completed successfully.\n",
      "Successfully processed sample_2.jpg\n",
      "Successfully queued sample_0.jpg\n",
      "Document 76e2478e-d44f-423e-960f-01c03a43752e (sample_0.jpg) processing completed successfully.\n",
      "Successfully processed sample_0.jpg\n",
      "Successfully queued sample_1.jpg\n",
      "Document b93f38fe-8296-44da-a678-6ab125b4f009 (sample_1.jpg) processing completed successfully.\n",
      "Successfully processed sample_1.jpg\n",
      "\n",
      "Processing complete:\n",
      "Successfully processed: 10\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Global tracking of documents being processed\n",
    "documents_in_process: Dict[str, Dict[str, Any]] = {}\n",
    "# Set to track documents that have been completed\n",
    "completed_documents: Set[str] = set()\n",
    "\n",
    "\n",
    "\n",
    "async def process_page(page, zoom: float = 1.5) -> Tuple[bytes, str]:\n",
    "    \"\"\"Process a single page to extract image and text\"\"\"\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    img_bytes = pix.tobytes(\"jpeg\")\n",
    "    text = page.get_text()\n",
    "    return img_bytes, text\n",
    "\n",
    "\n",
    "\n",
    "async def pdf_to_images_and_text(pdf_path: str) -> Tuple[List[bytes], List[str]]:\n",
    "    \"\"\"Convert PDF to list of images and text with better error handling\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    try:\n",
    "        # Process pages in chunks to avoid memory issues\n",
    "        chunk_size = 4\n",
    "        pages = [page for page in doc]\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(pages), chunk_size):\n",
    "            chunk = pages[i:i + chunk_size]\n",
    "            try:\n",
    "                chunk_results = await asyncio.gather(\n",
    "                    *(process_page(page) for page in chunk),\n",
    "                    return_exceptions=True\n",
    "                )\n",
    "                \n",
    "                # Handle any exceptions in processing\n",
    "                valid_results = []\n",
    "                for j, result in enumerate(chunk_results):\n",
    "                    if isinstance(result, Exception):\n",
    "                        print(f\"Error processing page {i+j}: {str(result)}\")\n",
    "                        # Add empty placeholder\n",
    "                        valid_results.append((b'', ''))\n",
    "                    else:\n",
    "                        valid_results.append(result)\n",
    "                        \n",
    "                results.extend(valid_results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {i//chunk_size}: {str(e)}\")\n",
    "                # Add empty placeholders for the whole chunk\n",
    "                results.extend([(b'', '') for _ in range(len(chunk))])\n",
    "            \n",
    "        images_bytes, texts = zip(*results) if results else ([], [])\n",
    "        return list(images_bytes), list(texts)\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "\n",
    "\n",
    "async def image_to_image_and_text(image_path: str) -> Tuple[List[bytes], List[str]]:\n",
    "    \"\"\"Convert an image file to a list with one image and empty text for DB compatibility\"\"\"\n",
    "    try:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "        \n",
    "        # Validate that this is actually an image\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(image_bytes))\n",
    "            # Convert to JPEG format for consistency with PDF processing\n",
    "            if img.format != 'JPEG':\n",
    "                byte_array = io.BytesIO()\n",
    "                # Convert to RGB if needed (for PNG with transparency, etc.)\n",
    "                if img.mode in ('RGBA', 'LA') or (img.mode == 'P' and 'transparency' in img.info):\n",
    "                    background = Image.new('RGB', img.size, (255, 255, 255))\n",
    "                    background.paste(img, mask=img if img.mode == 'RGBA' else None)\n",
    "                    img = background\n",
    "                img.save(byte_array, format='JPEG', optimize=True)\n",
    "                image_bytes = byte_array.getvalue()\n",
    "            img.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error validating image: {str(e)}\")\n",
    "            raise ValueError(f\"Invalid image file: {image_path}\")\n",
    "            \n",
    "        # Return as a list to be compatible with PDF processing\n",
    "        return [image_bytes], [\"\"]  # Empty text as placeholder\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image file {image_path}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def create_thumbnail(image_data: bytes, size=(400, 400)) -> bytes:\n",
    "    \"\"\"Create thumbnail from image bytes with error handling\"\"\"\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        image.thumbnail(size)\n",
    "        byte_array = io.BytesIO()\n",
    "        image.save(byte_array, format='JPEG', optimize=True)\n",
    "        return byte_array.getvalue()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating thumbnail: {str(e)}\")\n",
    "        # Return a minimal valid JPEG as fallback\n",
    "        return b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x06\\x07\\x06\\x05\\x08\\x07\\x07\\x07\\t\\t\\x08\\n\\x0c\\x14\\r\\x0c\\x0b\\x0b\\x0c\\x19\\x12\\x13\\x0f\\x14\\x1d\\x1a\\x1f\\x1e\\x1d\\x1a\\x1c\\x1c $.\\' \",#\\x1c\\x1c(7),01444\\x1f\\'9=82<.342\\xff\\xdb\\x00C\\x01\\t\\t\\t\\x0c\\x0b\\x0c\\x18\\r\\r\\x182!\\x1c!22222222222222222222222222222222222222222222222222\\xff\\xc0\\x00\\x11\\x08\\x00\\x01\\x00\\x01\\x03\\x01\"\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1f\\x00\\x00\\x01\\x05\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\xff\\xc4\\x00\\xb5\\x10\\x00\\x02\\x01\\x03\\x03\\x02\\x04\\x03\\x05\\x05\\x04\\x04\\x00\\x00\\x01}\\x01\\x02\\x03\\x00\\x04\\x11\\x05\\x12!1A\\x06\\x13Qa\\x07\"q\\x142\\x81\\x91\\xa1\\x08#B\\xb1\\xc1\\x15R\\xd1\\xf0$3br\\x82\\t\\n\\x16\\x17\\x18\\x19\\x1a%&\\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xe1\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xf1\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xff\\xc4\\x00\\x1f\\x01\\x00\\x03\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\xff\\xc4\\x00\\xb5\\x11\\x00\\x02\\x01\\x02\\x04\\x04\\x03\\x04\\x07\\x05\\x04\\x04\\x00\\x01\\x02w\\x00\\x01\\x02\\x03\\x11\\x04\\x05!1\\x06\\x12AQ\\x07aq\\x13\"2\\x81\\x08\\x14B\\x91\\xa1\\xb1\\xc1\\t#3R\\xf0\\x15br\\xd1\\n\\x16$4\\xe1%\\xf1\\x17\\x18\\x19\\x1a&\\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\\x82\\x83\\x84\\x85\\x86\\x87\\x88\\x89\\x8a\\x92\\x93\\x94\\x95\\x96\\x97\\x98\\x99\\x9a\\xa2\\xa3\\xa4\\xa5\\xa6\\xa7\\xa8\\xa9\\xaa\\xb2\\xb3\\xb4\\xb5\\xb6\\xb7\\xb8\\xb9\\xba\\xc2\\xc3\\xc4\\xc5\\xc6\\xc7\\xc8\\xc9\\xca\\xd2\\xd3\\xd4\\xd5\\xd6\\xd7\\xd8\\xd9\\xda\\xe2\\xe3\\xe4\\xe5\\xe6\\xe7\\xe8\\xe9\\xea\\xf2\\xf3\\xf4\\xf5\\xf6\\xf7\\xf8\\xf9\\xfa\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\xfe\\xfe(\\xa2\\x8a\\x00\\xff\\xd9'\n",
    "\n",
    "\n",
    "\n",
    "async def save_to_local(file_content: bytes, filename: str):\n",
    "    \"\"\"\n",
    "    Save the file content to a local directory.\n",
    "\n",
    "    Args:\n",
    "        file_content (bytes): The content of the file to save.\n",
    "        filename (str): The filename to use for saving, including extension (e.g., '123.pdf' or '456_full.jpg').\n",
    "    \"\"\"\n",
    "    # Ensure the storage directory exists\n",
    "    os.makedirs(PROCESSED_FOLDER, exist_ok=True)\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(PROCESSED_FOLDER, filename)\n",
    "    \n",
    "    # Use a helper function via run_in_executor to avoid blocking\n",
    "    def write_file():\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(file_content)\n",
    "    \n",
    "    # Run the synchronous file operation in a thread pool\n",
    "    loop = asyncio.get_running_loop()\n",
    "    await loop.run_in_executor(None, write_file)\n",
    "    \n",
    "    print(f\"File saved locally: {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def check_and_resize_for_vect(image_bytes: bytes, max_pixels: int = 1000000) -> bytes:\n",
    "    \"\"\"Resize image if needed for vector embedding with error handling\"\"\"\n",
    "    try:\n",
    "        if not image_bytes:\n",
    "            print(\"Warning: Empty image bytes provided\")\n",
    "            return image_bytes\n",
    "            \n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        current_pixels = image.width * image.height\n",
    "        \n",
    "        if current_pixels > max_pixels:\n",
    "            scale_factor = (max_pixels / current_pixels) ** 0.5\n",
    "            new_width = int(image.width * scale_factor)\n",
    "            new_height = int(image.height * scale_factor)\n",
    "            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            buffer = io.BytesIO()\n",
    "            image.save(buffer, format='JPEG', optimize=True)\n",
    "            return buffer.getvalue()\n",
    "        \n",
    "        return image_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Error resizing image: {str(e)}\")\n",
    "        return image_bytes  # Return original bytes on error\n",
    "\n",
    "\n",
    "\n",
    "async def verify_vector_in_database(page_id: UUID) -> bool:\n",
    "    async with aiosqlite.connect(LOCAL_DB_PATH) as conn:\n",
    "        await conn._execute(conn._conn.enable_load_extension, True)\n",
    "        await conn._execute(sqlite_vec.load, conn._conn)\n",
    "\n",
    "        conn.row_factory = aiosqlite.Row\n",
    "        query = \"\"\"\n",
    "            SELECT EXISTS(\n",
    "                SELECT 1\n",
    "                FROM page_images_vectors\n",
    "                WHERE page_id = ?\n",
    "            ) as has_vector\n",
    "        \"\"\"\n",
    "        async with conn.execute(query, (str(page_id),)) as cursor:\n",
    "            result = await cursor.fetchone()\n",
    "            await conn._execute(conn._conn.enable_load_extension, False)\n",
    "            return result['has_vector'] == 1\n",
    "\n",
    "\n",
    "\n",
    "async def retry_missing_page_vectors(document_id: UUID, page_ids: List[UUID], images: List[bytes], max_retries: int = 3) -> int:\n",
    "    \"\"\"Retry storing vectors for pages that are missing them\"\"\"\n",
    "    retry_count = 0\n",
    "    fixed_pages = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        missing_vectors = []\n",
    "        \n",
    "        # Check which pages are missing vectors\n",
    "        for idx, page_id in enumerate(page_ids):\n",
    "            if not await verify_vector_in_database(page_id):\n",
    "                if idx < len(images) and images[idx]:  # Make sure we have image data\n",
    "                    missing_vectors.append((idx, page_id))\n",
    "        \n",
    "        if not missing_vectors:\n",
    "            print(f\"All vectors present for document {document_id} after retries\")\n",
    "            return fixed_pages\n",
    "            \n",
    "        print(f\"Found {len(missing_vectors)} pages with missing vectors for document {document_id}, retry {retry_count + 1}/{max_retries}\")\n",
    "        \n",
    "        # Retry each missing vector\n",
    "        for idx, page_id in missing_vectors:\n",
    "            try:\n",
    "                # Resize image for vector embedding\n",
    "                resized_image = check_and_resize_for_vect(images[idx])\n",
    "                \n",
    "                # Queue image for embedding with higher priority\n",
    "                await embedding_queue.add_image_task(\n",
    "                    str(document_id),\n",
    "                    \"Retry\", # Title doesn't matter for retries\n",
    "                    resized_image,\n",
    "                    idx,\n",
    "                    len(page_ids),\n",
    "                    str(page_id),\n",
    "                    priority=True  # Mark as high priority retry\n",
    "                )\n",
    "                \n",
    "                print(f\"Requeued page {idx} (ID: {page_id}) for document {document_id}\")\n",
    "                fixed_pages += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error requeuing page {idx} for document {document_id}: {str(e)}\")\n",
    "        \n",
    "        # Wait before checking again\n",
    "        retry_delay = 60 * (retry_count + 1)  # Increase wait time with each retry\n",
    "        print(f\"Waiting {retry_delay} seconds before checking vectors again...\")\n",
    "        await asyncio.sleep(retry_delay)\n",
    "        retry_count += 1\n",
    "    \n",
    "    return fixed_pages\n",
    "\n",
    "\n",
    "\n",
    "async def verify_document_vector(document_id: UUID) -> bool:\n",
    "    async with aiosqlite.connect(LOCAL_DB_PATH) as conn:\n",
    "        await conn._execute(conn._conn.enable_load_extension, True)\n",
    "        await conn._execute(sqlite_vec.load, conn._conn)\n",
    "\n",
    "        conn.row_factory = aiosqlite.Row\n",
    "        query = \"\"\"\n",
    "            SELECT \n",
    "                (SELECT COUNT(*) FROM page_images WHERE document_id = ?) as total_pages,\n",
    "                (SELECT COUNT(*) FROM page_images_vectors piv \n",
    "                 JOIN page_images pi ON pi.page_id = piv.page_id \n",
    "                 WHERE pi.document_id = ?) as pages_with_vectors\n",
    "        \"\"\"\n",
    "        async with conn.execute(query, (str(document_id), str(document_id))) as cursor:\n",
    "            result = await cursor.fetchone()\n",
    "            await conn._execute(conn._conn.enable_load_extension, False)\n",
    "            \n",
    "            if not result or result['total_pages'] == 0:\n",
    "                return False\n",
    "                \n",
    "            # Document is considered complete if at least 90% of pages have vectors\n",
    "            return (result['pages_with_vectors'] / result['total_pages']) >= 0.9\n",
    "\n",
    "\n",
    "async def compute_and_store_document_vector(document_id: UUID) -> bool:\n",
    "    async with aiosqlite.connect(LOCAL_DB_PATH) as conn:\n",
    "        await conn._execute(conn._conn.enable_load_extension, True)\n",
    "        await conn._execute(sqlite_vec.load, conn._conn)\n",
    "\n",
    "        conn.row_factory = aiosqlite.Row\n",
    "        \n",
    "        # Check if page vectors exist\n",
    "        vector_query = \"\"\"\n",
    "            SELECT COUNT(*) as vector_count\n",
    "            FROM page_images_vectors piv\n",
    "            JOIN page_images pi ON piv.page_id = pi.page_id\n",
    "            WHERE pi.document_id = ?\n",
    "        \"\"\"\n",
    "        async with conn.execute(vector_query, (str(document_id),)) as cursor:\n",
    "            result = await cursor.fetchone()\n",
    "        \n",
    "        if not result or result['vector_count'] == 0:\n",
    "            print(f\"No page vectors found for document {document_id}\")\n",
    "            await conn._execute(conn._conn.enable_load_extension, False)\n",
    "            return False\n",
    "        \n",
    "        # Count total pages\n",
    "        pages_query = \"\"\"\n",
    "            SELECT COUNT(*) as page_count\n",
    "            FROM page_images\n",
    "            WHERE document_id = ?\n",
    "        \"\"\"\n",
    "        async with conn.execute(pages_query, (str(document_id),)) as cursor:\n",
    "            pages_result = await cursor.fetchone()\n",
    "            \n",
    "        total_pages = pages_result['page_count'] if pages_result else 0\n",
    "        vectorized_pages = result['vector_count']\n",
    "        \n",
    "        # Document is considered successfully processed if at least 90% of pages have vectors\n",
    "        success = total_pages > 0 and (vectorized_pages / total_pages) >= 0.9\n",
    "        \n",
    "        if success:\n",
    "            print(f\"Document {document_id} successfully processed: {vectorized_pages}/{total_pages} pages vectorized\")\n",
    "            # Mark document as completed in memory\n",
    "            completed_documents.add(str(document_id))\n",
    "        else:\n",
    "            print(f\"Document {document_id} not fully processed: {vectorized_pages}/{total_pages} pages vectorized\")\n",
    "            \n",
    "        await conn._execute(conn._conn.enable_load_extension, False)\n",
    "        return success\n",
    "\n",
    "\n",
    "\n",
    "async def monitor_document_progress(document_id: UUID, filename: str, images: List[bytes], page_ids: List[UUID], total_timeout: int = 3600):\n",
    "    \"\"\"Monitor progress of document processing with timeout and verify vectors exist\"\"\"\n",
    "    doc_str = str(document_id)\n",
    "    start_time = time.time()\n",
    "    last_check_time = start_time\n",
    "    last_processed = 0\n",
    "    \n",
    "    print(f\"Starting monitoring for document {doc_str} ({filename})\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get document status\n",
    "            status = await get_document_processing_status(embedding_queue, doc_str)\n",
    "            \n",
    "            current_time = time.time()\n",
    "            elapsed = current_time - start_time\n",
    "            \n",
    "            # If document is not found in queue or database\n",
    "            if not status:\n",
    "                if elapsed > 300:  # After 5 minutes, if not found, we have a problem\n",
    "                    print(f\"Document {doc_str} not found in queue or database after 5 minutes. Possible error.\")\n",
    "                    return False\n",
    "                await asyncio.sleep(30)\n",
    "                continue\n",
    "                \n",
    "            # If document is completed according to queue\n",
    "            if status.get(\"status\") == \"completed\":\n",
    "                # Verify document vector exists\n",
    "                if not await verify_document_vector(document_id):\n",
    "                    print(f\"Document {doc_str} marked as completed but missing document vector. Computing now.\")\n",
    "                    if await compute_and_store_document_vector(document_id):\n",
    "                        print(f\"Successfully created document vector for {doc_str}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to create document vector for {doc_str}\")\n",
    "                        # Try to fix missing page vectors first\n",
    "                        fixed_pages = await retry_missing_page_vectors(document_id, page_ids, images)\n",
    "                        if fixed_pages > 0:\n",
    "                            # Wait for vectors to be processed\n",
    "                            await asyncio.sleep(60)\n",
    "                            # Try computing document vector again\n",
    "                            if await compute_and_store_document_vector(document_id):\n",
    "                                print(f\"Successfully created document vector for {doc_str} after fixing page vectors\")\n",
    "                            else:\n",
    "                                print(f\"Still failed to create document vector for {doc_str}\")\n",
    "                                return False\n",
    "                \n",
    "                print(f\"Document {doc_str} ({filename}) processing completed successfully.\")\n",
    "                completed_documents.add(doc_str)\n",
    "                return True\n",
    "                \n",
    "            # Check progress\n",
    "            total_pages = status.get(\"total_pages\", 0)\n",
    "            processed = status.get(\"processed_pages\", 0)\n",
    "            failed = status.get(\"failed_pages\", 0)\n",
    "            progress = status.get(\"progress_percentage\", 0)\n",
    "            \n",
    "            # Calculate processing rate\n",
    "            time_since_last = current_time - last_check_time\n",
    "            if time_since_last >= 60:  # Update every minute\n",
    "                pages_per_minute = (processed - last_processed) / (time_since_last / 60) if time_since_last > 0 else 0\n",
    "                last_check_time = current_time\n",
    "                last_processed = processed\n",
    "                \n",
    "                # Calculate estimated time remaining\n",
    "                pages_remaining = total_pages - processed\n",
    "                estimated_minutes = pages_remaining / max(0.1, pages_per_minute) if pages_per_minute > 0 else float('inf')\n",
    "                \n",
    "                print(f\"\"\"Document {doc_str} ({filename}) progress:\n",
    "                    - Elapsed time: {elapsed:.0f} seconds\n",
    "                    - Pages: {processed}/{total_pages} ({progress:.1f}%)\n",
    "                    - Failed pages: {failed}\n",
    "                    - Processing rate: {pages_per_minute:.1f} pages/minute\n",
    "                    - Estimated remaining: {estimated_minutes:.1f} minutes\"\"\")\n",
    "                \n",
    "                # If no progress for 5 minutes and we have processed pages, retry missing vectors\n",
    "                if pages_per_minute < 0.1 and processed > 0 and elapsed > 300:\n",
    "                    print(f\"Document {doc_str} processing appears stalled. Retrying missing vectors.\")\n",
    "                    fixed_pages = await retry_missing_page_vectors(document_id, page_ids, images)\n",
    "                    if fixed_pages > 0:\n",
    "                        print(f\"Requeued {fixed_pages} pages for vector processing\")\n",
    "            \n",
    "            # Check for timeout\n",
    "            if elapsed > total_timeout:\n",
    "                print(f\"Document {doc_str} processing timed out after {total_timeout} seconds.\")\n",
    "                # Force document vector calculation if we have enough pages\n",
    "                if processed / total_pages >= 0.7:  # If at least 70% complete, calculate vector\n",
    "                    print(f\"Document {doc_str} has {processed}/{total_pages} pages processed. Forcing vector calculation.\")\n",
    "                    \n",
    "                    # Try to fix missing page vectors first\n",
    "                    fixed_pages = await retry_missing_page_vectors(document_id, page_ids, images)\n",
    "                    \n",
    "                    # Wait a bit for vectors to be processed\n",
    "                    if fixed_pages > 0:\n",
    "                        print(f\"Waiting for {fixed_pages} requeued vectors to be processed...\")\n",
    "                        await asyncio.sleep(60)\n",
    "                    \n",
    "                    # Calculate document vector\n",
    "                    if await compute_and_store_document_vector(document_id):\n",
    "                        print(f\"Successfully created document vector for timed out document {doc_str}\")\n",
    "                        completed_documents.add(doc_str)\n",
    "                        return True\n",
    "                    else:\n",
    "                        print(f\"Failed to create document vector for timed out document {doc_str}\")\n",
    "                        return False\n",
    "                return False\n",
    "                \n",
    "            # Sleep before next check\n",
    "            await asyncio.sleep(30)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error monitoring document {doc_str}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            await asyncio.sleep(30)\n",
    "\n",
    "\n",
    "\n",
    "async def process_single_pdf(pdf_path: str, timeout: int = 3600):\n",
    "    \"\"\"Process a single PDF file with consistent IDs across folder and DB\"\"\"\n",
    "    try:\n",
    "        # Generate document ID - will be used consistently in both folder and DB\n",
    "        document_id = uuid4()\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        print(f\"Processing {pdf_path}\")\n",
    "        \n",
    "        # Track start time\n",
    "        start_time = time.time()\n",
    "        documents_in_process[str(document_id)] = {\n",
    "            \"filename\": filename,\n",
    "            \"start_time\": start_time,\n",
    "            \"status\": \"extracting\"\n",
    "        }\n",
    "\n",
    "        # Extract images and text from PDF\n",
    "        images, text_pages = await pdf_to_images_and_text(pdf_path)\n",
    "        total_pages = len(images)\n",
    "\n",
    "        print(f\"Extracted {total_pages} images and text from {filename}\")\n",
    "        \n",
    "        documents_in_process[str(document_id)][\"status\"] = \"metadata\"\n",
    "        documents_in_process[str(document_id)][\"total_pages\"] = total_pages\n",
    "\n",
    "        # Store PDF to folder with document_id\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"storing\"\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            pdf_content = f.read()\n",
    "        await save_to_local(pdf_content, f\"{document_id}.pdf\")\n",
    "\n",
    "        # Generate page_ids before processing to ensure consistency between folder and DB\n",
    "        page_ids = [str(uuid4()) for _ in range(total_pages)]\n",
    "        \n",
    "        # Store in database with page_ids first to ensure DB entries exist\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"database_init\"\n",
    "        await db_store_pdf_file(\n",
    "            document_id=str(document_id),\n",
    "            title=filename,\n",
    "            page_texts=text_pages,\n",
    "            page_ids=page_ids\n",
    "        )\n",
    "        \n",
    "        # Track successfully processed pages\n",
    "        processed_pages = []\n",
    "        \n",
    "        # Process and store images to folder\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"image_processing\"\n",
    "        for page_idx, (image_data, page_id) in enumerate(zip(images, page_ids)):\n",
    "            # Skip empty images (from extraction errors)\n",
    "            if not image_data:\n",
    "                print(f\"Warning: Empty image data for page {page_idx}. Skipping folder storing.\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Use consistent page_id for both folder and database\n",
    "                # store full resolution image\n",
    "                await save_to_local(image_data, f\"{page_id}_full.jpg\")\n",
    "\n",
    "                # Create and store thumbnail\n",
    "                thumb_data = create_thumbnail(image_data)\n",
    "                await save_to_local(thumb_data, f\"{page_id}_thumb.jpg\")\n",
    "\n",
    "                # Queue image for embedding with the specific page_id\n",
    "                resized_image = check_and_resize_for_vect(image_data)\n",
    "                await embedding_queue.add_image_task(\n",
    "                    str(document_id),\n",
    "                    filename,\n",
    "                    resized_image,\n",
    "                    page_idx,\n",
    "                    total_pages,\n",
    "                    str(page_id)  # Pass page_id to ensure consistency\n",
    "                )\n",
    "                \n",
    "                processed_pages.append(page_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_idx} for {filename}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                # Continue with other pages despite error\n",
    "\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"monitoring\"\n",
    "        # Launch background task to monitor progress (don't await it)\n",
    "        monitor_task = asyncio.create_task(\n",
    "            monitor_document_progress(document_id, filename, images, page_ids, timeout)\n",
    "        )\n",
    "        documents_in_process[str(document_id)][\"monitor_task\"] = monitor_task\n",
    "\n",
    "        print(f\"Successfully queued {filename} for processing\")\n",
    "        return document_id, monitor_task\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "async def process_single_image(image_path: str, timeout: int = 3600):\n",
    "    \"\"\"Process a single image file as a one-page document\"\"\"\n",
    "    try:\n",
    "        # Generate document ID - will be used consistently in both folder and DB\n",
    "        document_id = uuid4()\n",
    "        filename = os.path.basename(image_path)\n",
    "        print(f\"Processing image {image_path}\")\n",
    "\n",
    "        latex_path = image_path.replace(\".jpg\", \".txt\")\n",
    "        print(f\"Looking for LaTeX file: {latex_path}\")\n",
    "\n",
    "        # Track start time\n",
    "        start_time = time.time()\n",
    "        documents_in_process[str(document_id)] = {\n",
    "            \"filename\": filename,\n",
    "            \"start_time\": start_time,\n",
    "            \"status\": \"extracting\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(latex_path, 'r', encoding='utf-8') as f:\n",
    "                latex_code = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading LaTeX file {latex_path}: {str(e)}\")\n",
    "\n",
    "        # Extract image bytes\n",
    "        images, text_pages = await image_to_image_and_text(image_path)\n",
    "        total_pages = 1  # Single page for images\n",
    "\n",
    "        print(f\"Extracted image as a single-page document: {filename}\")\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"metadata\"\n",
    "        documents_in_process[str(document_id)][\"total_pages\"] = total_pages\n",
    "\n",
    "        documents_in_process[str(document_id)][\"metadata\"] = {\n",
    "            \"title\": filename,\n",
    "        }\n",
    "\n",
    "        # Store original image to folder with document_id\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"storing\"\n",
    "        \n",
    "        # Determine the file extension from the original file\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        await save_to_local(images[0], f\"{document_id}{file_ext}\")\n",
    "\n",
    "        # Generate page_id for consistency between folder and DB\n",
    "        page_ids = [str(uuid4())]\n",
    "        \n",
    "        # Store in database with page_id first to ensure DB entry exists\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"database_init\"\n",
    "        await db_store_pdf_file(\n",
    "            document_id=str(document_id),\n",
    "            title=filename,\n",
    "            page_texts=text_pages,\n",
    "            page_ids=page_ids,\n",
    "            latex_code=latex_code\n",
    "        )\n",
    "        \n",
    "        # Process and store images to folder\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"image_processing\"\n",
    "        \n",
    "        try:\n",
    "            # Use consistent page_id for both folder and database\n",
    "            # store full resolution image\n",
    "            await save_to_local(images[0], f\"{page_ids[0]}_full.jpg\")\n",
    "\n",
    "            # Create and store thumbnail\n",
    "            thumb_data = create_thumbnail(images[0])\n",
    "            await save_to_local(thumb_data, f\"{page_ids[0]}_thumb.jpg\")\n",
    "\n",
    "            # Queue image for embedding with the specific page_id\n",
    "            resized_image = check_and_resize_for_vect(images[0])\n",
    "            await embedding_queue.add_image_task(\n",
    "                str(document_id),\n",
    "                filename,\n",
    "                resized_image,\n",
    "                0,  # page_idx\n",
    "                total_pages,\n",
    "                str(page_ids[0])  # Pass page_id to ensure consistency\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {filename}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        documents_in_process[str(document_id)][\"status\"] = \"monitoring\"\n",
    "        # Launch background task to monitor progress (don't await it)\n",
    "        monitor_task = asyncio.create_task(\n",
    "            monitor_document_progress(document_id, filename, images, page_ids, timeout)\n",
    "        )\n",
    "        documents_in_process[str(document_id)][\"monitor_task\"] = monitor_task\n",
    "\n",
    "        print(f\"Successfully queued image {filename} for processing\")\n",
    "        return document_id, monitor_task\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "async def process_pdf_folder(folder_path: str, concurrent_limit: int = 2, timeout: int = 3600):\n",
    "    \"\"\"Process PDFs from subfolders, using subfolder names with concurrency control\"\"\"\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    pending_tasks = []\n",
    "    \n",
    "    # Create a semaphore to limit concurrent processing\n",
    "    semaphore = asyncio.Semaphore(concurrent_limit)\n",
    "    \n",
    "    async def process_with_semaphore(file_path):\n",
    "        async with semaphore:\n",
    "            # Check file extension to determine processing method\n",
    "            file_ext = os.path.splitext(file_path)[1].lower()\n",
    "            if file_ext == '.pdf':\n",
    "                return await process_single_pdf(file_path, timeout)\n",
    "            elif file_ext in ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'):\n",
    "                return await process_single_image(file_path, timeout)\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {file_ext}\")\n",
    "                return None, None\n",
    "    \n",
    "    # Walk through all subfolders\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        # Find PDF, image, and text files\n",
    "        pdf_files = [f for f in files if f.lower().endswith('.pdf')]\n",
    "        image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'))]\n",
    "        total_files = len(pdf_files) + len(image_files)\n",
    "        \n",
    "        if not total_files:\n",
    "            print(f\"No PDF or image files found in folder: {os.path.basename(root)}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {len(pdf_files)} PDF files, {len(image_files)} image files from folder: {os.path.basename(root)}\")\n",
    "        \n",
    "        # Launch tasks for all PDFs in this folder\n",
    "        for pdf_file in pdf_files:\n",
    "            file_path = os.path.join(root, pdf_file)\n",
    "            print(f\"\\nQueuing PDF: {pdf_file}\")\n",
    "            \n",
    "            # Create and start the task\n",
    "            task = asyncio.create_task(process_with_semaphore(file_path))\n",
    "            pending_tasks.append((pdf_file, task))\n",
    "        \n",
    "        # Launch tasks for all image files in this folder\n",
    "        for image_file in image_files:\n",
    "            file_path = os.path.join(root, image_file)\n",
    "            print(f\"\\nQueuing image: {image_file}\")\n",
    "            \n",
    "            # Create and start the task\n",
    "            task = asyncio.create_task(process_with_semaphore(file_path))\n",
    "            pending_tasks.append((image_file, task))\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    for file_name, task in pending_tasks:\n",
    "        try:\n",
    "            doc_id, monitor_task = await task\n",
    "            \n",
    "            if doc_id:\n",
    "                print(f\"Successfully queued {file_name}\")\n",
    "                # Wait for monitoring to complete\n",
    "                if monitor_task:\n",
    "                    result = await monitor_task\n",
    "                    if result:\n",
    "                        successful += 1\n",
    "                        print(f\"Successfully processed {file_name}\")\n",
    "                    else:\n",
    "                        # Even if monitoring failed, the document might have been processed\n",
    "                        if str(doc_id) in completed_documents:\n",
    "                            successful += 1\n",
    "                            print(f\"Document {file_name} was eventually processed successfully\")\n",
    "                        else:\n",
    "                            failed += 1\n",
    "                            print(f\"Failed to fully process {file_name}\")\n",
    "                else:\n",
    "                    # No monitoring task means something failed early\n",
    "                    failed += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"Failed to process {file_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    return successful, failed\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main async function to run the whole process\"\"\"\n",
    "    # Folder containing PDFs and images\n",
    "    DOCS_FOLDER = EXTRACTION_FOLDER\n",
    "    \n",
    "    # Make sure the embedding queue background tasks are started\n",
    "    # This will work because we're already inside an event loop (asyncio.run creates one)\n",
    "    embedding_queue.start_background_tasks()\n",
    "    \n",
    "    # Run the processor\n",
    "    success, failed = await process_pdf_folder(DOCS_FOLDER, concurrent_limit=2, timeout=3600)\n",
    "    \n",
    "    return success, failed\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run everything in a single asyncio.run() call\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2903229",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
