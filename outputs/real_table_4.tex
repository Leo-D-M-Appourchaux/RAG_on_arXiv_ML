\documentclass{article}%
\usepackage[T1]{fontenc}%
\usepackage[utf8]{inputenc}%
\usepackage{lmodern}%
\usepackage{textcomp}%
\usepackage{lastpage}%
\usepackage{float}%
\usepackage{booktabs}%
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{makecell}%
\usepackage{cite}%
\usepackage{threeparttable}%
\usepackage{xcolor}%
\usepackage{amssymb}%
\usepackage{hyperref}%
\usepackage{textcomp}%
%
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}%
%
\begin{document}%
\normalsize%
\begin{table}[htbp]
% \caption{DNN resource requirements modeling. ASIC: Application-Specific Integrated Circuit. Matmul: matrix multiplication. RMSPE: root mean square percentage error.}
% %\centering
% %\begin{tabular}{ m{1.2cm}<{\centering}|m{1.6cm}<{\centering}| m{1.6cm}<{\centering}| m{1.8cm}<{\centering}|m{2.4cm}<{\centering}|m{2.2cm}<{\centering}|m{1.4cm}<{\centering}}
% \begin{tabular}{ m{1.6cm}|m{1.6cm}| m{1.5cm}| m{1.4cm}|m{2.8cm}|m{1.4cm}|m{1.8cm}}
% %\begin{tabularx}{1\linewidth}{X| X| X| X| X| X| X}
% \toprule
% Work & Platform & Framework & Metric & Measured features & Regression model & Relative ~~~~~~~~~~~~error \\
% \noalign{
% \hrule height 2pt
% }
% Augur\cite{lu2017modeling} & NVidia TK1, TX1 &Caffe &inference: memory, time &matrix dimensions in matmul, weights, activations&linear & memory: 28\% - 50\%; time: 6\% - 20\%\\
% \hline
% Paleo\cite{qi2016paleo} & NVidia Titan X GPU cluster & TensorFlow&training \& inference: time&forward \& backward FLOPs, weights, activations, data, platform percent of peak&linear& 4\%-30\% \\
% \hline
% Gianniti et al.\cite{giannitiperformance}&NVidia Quadro M6000 GPU & - &training: time & forward \& backward FLOPs of all types of layers & linear & < 23\% \\
% \hline
% SyNERGY\cite{rodriguesfine} & Nvidia Jetson TX1 & Caffe & inference: energy & MACs & linear & < 17\% (w/o MobileNet)\\
% \hline
% NeuralPower\cite{cai2017neuralpower}&Nvidia Titan X \& GTX 1070 &TensorFlow \& Caffe &inference: time, power, energy&layer configuration hyper-parameters, memory access, FLOPs, activations, batch size&polynomial&time: < 24\%; power: < 20\%; energy: < 5\%\\
% \hline
% HyperPower\cite{stamoulis2018hyperpower}&Nvidia GTX1070 \& Tegra TX1&Caffe&inference: power, memory&layer configuration hyper-parameters&linear & RMSPE < 7\%\\
% \hline
% Yang et al.\cite{yang2017designing} & ASIC Eyeriss\cite{chen2017eyeriss} & - & inference: energy & MACs, memory access&- & -\\
% \hline
% DeLight\cite{rouhani2016delight} & Nvidia Tegra TK1& Theano  & training\& inference: energy & layer configuration hyper-parameters & linear & -\\
% \bottomrule
% \end{tabular}
% \label{table:DNNresourcemodeling}
% \end{table}%
\end{document}