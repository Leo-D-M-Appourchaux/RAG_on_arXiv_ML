\documentclass{article}%
\usepackage[T1]{fontenc}%
\usepackage[utf8]{inputenc}%
\usepackage{lmodern}%
\usepackage{textcomp}%
\usepackage{lastpage}%
\usepackage{float}%
\usepackage{booktabs}%
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{makecell}%
\usepackage{cite}%
\usepackage{threeparttable}%
\usepackage{xcolor}%
\usepackage{amssymb}%
\usepackage{hyperref}%
\usepackage{textcomp}%
%
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}%
%
\begin{document}%
\normalsize%
\begin{table}[htbp]
% \caption{Comparison of traditional machine learning algorithms. Notation: $m$-number of training samples; $n$-input dimension; $c$-number of classes.}
% \centering
% \begin{tabular}{ m{2.1cm}<{\centering}|m{1.8cm}<{\centering}| m{3.2cm}<{\centering}|m{2.8cm}<{\centering}|m{3.0cm}<{\centering}}
% \toprule
% Algorithm & Model size & Optimization & Training complexity & Inference complexity\\
% \noalign{
% \hrule height 2pt
% }
% Decision tree &$\mathcal{O}(m)$ &- &$\mathcal{O}(mnlog(m))$ & $\mathcal{O}(log(m))$\\
% \hline
% Random forest & $\mathcal{O}(N_{tree}m)$ &- &$\mathcal{O}(N_{tree}mnlog(m))$ & $\mathcal{O}(N_{tree}log(m))$\\
% \hline
% SVM & $\mathcal{O}(n)$ & gradient descent &$\mathcal{O}(m^2n)$ &$\mathcal{O}(m_{sv}n)$ \\
% \hline
% Logistic regression & $\mathcal{O}(n)$ &Newton-Raphson &$\mathcal{O}(mn^2+n^3)$ & $\mathcal{O}(n)$ \\
% \hline
% kNN &$\mathcal{O}(mn)$&- & - &$\mathcal{O}(mn)$ \\
% \hline
% Naive Bayes &$\mathcal{O}(nc)$ &-&$\mathcal{O}(mn+nc)$ & $\mathcal{O}(nc)$ \\
% \hline
% Linear regression &$\mathcal{O}(n)$ &matrix inversion &$\mathcal{O}(mn^2+n^3)$ &$\mathcal{O}(n)$ \\
% \noalign{
% \hrule height 2pt
% }
% k-Means &- &- &$\mathcal{O}(mnc)$ &- \\
% \hline
% EM &-&-&$\mathcal{O}(mn^2+n^3)$ & -\\
% \noalign{
% \hrule height 2pt
% }
% PCA &- &eigen-decomposition &$\mathcal{O}(mn^2+n^3)$ & -\\
% \bottomrule
% \end{tabular}
% \label{table:MLcompare}
% \end{table}%
\end{document}